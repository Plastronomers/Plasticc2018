{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bahar-NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnNvg83auDfe",
        "colab_type": "code",
        "outputId": "8759e47d-d084-48db-c1bb-93e20ba5fcba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFDOdJFQuRPM",
        "colab_type": "code",
        "outputId": "e309da43-11f3-44f6-b2a2-245c4d2d2a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-uUmu9umsm",
        "colab_type": "code",
        "outputId": "5c870a50-e47e-430b-c31a-561c73e36e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from astropy.io import fits\n",
        "from astropy.table import Table\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, Flatten , BatchNormalization , ReLU\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras as K"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjb5jr-fPWN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conf_plotter(y_true, y_pred, classes,\n",
        "                      normalize=0,\n",
        "                      title=None,\n",
        "                      cmap=plt.cm.pink_r,\n",
        "                      mode = 'train',\n",
        "                      save = 0,\n",
        "                      name = 'conf.jpg'):\n",
        "\n",
        "    if not title:\n",
        "        if mode == 'train':\n",
        "            title = 'Normalized confusion matrix on train dataset'\n",
        "        else:\n",
        "            title = 'Normalized confusion matrix on test dataset'\n",
        "            \n",
        "\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax ,fraction=0.046, pad=0.04)\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           xlabel='Predicted label',\n",
        "           ylabel='True label')\n",
        "    \n",
        "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] ):\n",
        "                    \n",
        "        item.set_fontsize(20)\n",
        "     \n",
        "    for item in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "        \n",
        "        item.set_fontsize(12)\n",
        "\n",
        "        \n",
        "                \n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    if save == 1:\n",
        "        plt.savefig(name)\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGx8LNX2uh9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('./daFeatures_train2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JnmSoMBuyKL",
        "colab_type": "code",
        "outputId": "20b2f20a-b58f-44a5-dc46-3cefc3128863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>object_id</th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_ratio_sq_sum</th>\n",
              "      <th>flux_ratio_sq_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>flux_w_mean</th>\n",
              "      <th>flux_diff1</th>\n",
              "      <th>flux_diff2</th>\n",
              "      <th>flux_diff3</th>\n",
              "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>0__kurtosis</th>\n",
              "      <th>0__skewness</th>\n",
              "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>1__kurtosis</th>\n",
              "      <th>1__skewness</th>\n",
              "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>2__kurtosis</th>\n",
              "      <th>2__skewness</th>\n",
              "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>3__kurtosis</th>\n",
              "      <th>3__skewness</th>\n",
              "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>...</th>\n",
              "      <th>__freq_signif_ratio_21___3_</th>\n",
              "      <th>__freq_signif_ratio_21___4_</th>\n",
              "      <th>__freq_signif_ratio_21___5_</th>\n",
              "      <th>__freq_signif_ratio_31___0_</th>\n",
              "      <th>__freq_signif_ratio_31___1_</th>\n",
              "      <th>__freq_signif_ratio_31___2_</th>\n",
              "      <th>__freq_signif_ratio_31___3_</th>\n",
              "      <th>__freq_signif_ratio_31___4_</th>\n",
              "      <th>__freq_signif_ratio_31___5_</th>\n",
              "      <th>__freq_varrat___0_</th>\n",
              "      <th>__freq_varrat___1_</th>\n",
              "      <th>__freq_varrat___2_</th>\n",
              "      <th>__freq_varrat___3_</th>\n",
              "      <th>__freq_varrat___4_</th>\n",
              "      <th>__freq_varrat___5_</th>\n",
              "      <th>__freq_y_offset___0_</th>\n",
              "      <th>__freq_y_offset___1_</th>\n",
              "      <th>__freq_y_offset___2_</th>\n",
              "      <th>__freq_y_offset___3_</th>\n",
              "      <th>__freq_y_offset___4_</th>\n",
              "      <th>__freq_y_offset___5_</th>\n",
              "      <th>time_score</th>\n",
              "      <th>phase_score</th>\n",
              "      <th>ddf_bool</th>\n",
              "      <th>true_target</th>\n",
              "      <th>true_submodel</th>\n",
              "      <th>true_z</th>\n",
              "      <th>true_distmod</th>\n",
              "      <th>true_lensdmu</th>\n",
              "      <th>true_vpec</th>\n",
              "      <th>true_rv</th>\n",
              "      <th>true_av</th>\n",
              "      <th>true_peakmjd</th>\n",
              "      <th>libid_cadence</th>\n",
              "      <th>tflux_u</th>\n",
              "      <th>tflux_g</th>\n",
              "      <th>tflux_r</th>\n",
              "      <th>tflux_i</th>\n",
              "      <th>tflux_z</th>\n",
              "      <th>tflux_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>615</td>\n",
              "      <td>-1100.440063</td>\n",
              "      <td>660.626343</td>\n",
              "      <td>-123.096998</td>\n",
              "      <td>-89.477524</td>\n",
              "      <td>394.109851</td>\n",
              "      <td>-0.349540</td>\n",
              "      <td>2.130510</td>\n",
              "      <td>12.845472</td>\n",
              "      <td>4.482743</td>\n",
              "      <td>3.835269</td>\n",
              "      <td>1.744747</td>\n",
              "      <td>1.623740</td>\n",
              "      <td>0.946023</td>\n",
              "      <td>2.929669e+06</td>\n",
              "      <td>0.812722</td>\n",
              "      <td>-9.601766e+08</td>\n",
              "      <td>-1.414322</td>\n",
              "      <td>-327.742307</td>\n",
              "      <td>1761.066406</td>\n",
              "      <td>-14.306331</td>\n",
              "      <td>-5.373326</td>\n",
              "      <td>205.036926</td>\n",
              "      <td>1628.427737</td>\n",
              "      <td>-1.475181</td>\n",
              "      <td>0.128917</td>\n",
              "      <td>22370.594834</td>\n",
              "      <td>2806.374162</td>\n",
              "      <td>-1.255123</td>\n",
              "      <td>0.415580</td>\n",
              "      <td>7780.500807</td>\n",
              "      <td>2805.598113</td>\n",
              "      <td>-1.409885</td>\n",
              "      <td>0.339918</td>\n",
              "      <td>7024.003068</td>\n",
              "      <td>2536.068846</td>\n",
              "      <td>-1.449858</td>\n",
              "      <td>0.293128</td>\n",
              "      <td>3245.366349</td>\n",
              "      <td>...</td>\n",
              "      <td>0.919127</td>\n",
              "      <td>0.936241</td>\n",
              "      <td>0.980085</td>\n",
              "      <td>0.750430</td>\n",
              "      <td>0.798199</td>\n",
              "      <td>0.734428</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.628905</td>\n",
              "      <td>0.915124</td>\n",
              "      <td>0.127550</td>\n",
              "      <td>0.269031</td>\n",
              "      <td>0.139362</td>\n",
              "      <td>0.110785</td>\n",
              "      <td>0.129578</td>\n",
              "      <td>0.401664</td>\n",
              "      <td>4.173310</td>\n",
              "      <td>-29.908400</td>\n",
              "      <td>18.747900</td>\n",
              "      <td>17.090800</td>\n",
              "      <td>20.412000</td>\n",
              "      <td>12.103300</td>\n",
              "      <td>0.197098</td>\n",
              "      <td>0.934001</td>\n",
              "      <td>1</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59570.000</td>\n",
              "      <td>69</td>\n",
              "      <td>484.7</td>\n",
              "      <td>3286.7</td>\n",
              "      <td>3214.1</td>\n",
              "      <td>3039.7</td>\n",
              "      <td>2854.5</td>\n",
              "      <td>2837.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>713</td>\n",
              "      <td>-14.735178</td>\n",
              "      <td>14.770886</td>\n",
              "      <td>-1.423351</td>\n",
              "      <td>-0.873033</td>\n",
              "      <td>6.471144</td>\n",
              "      <td>0.014989</td>\n",
              "      <td>0.639458</td>\n",
              "      <td>9.115748</td>\n",
              "      <td>2.359620</td>\n",
              "      <td>1.998217</td>\n",
              "      <td>1.509888</td>\n",
              "      <td>1.633246</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>5.886068e+03</td>\n",
              "      <td>3.439423</td>\n",
              "      <td>-2.875087e+04</td>\n",
              "      <td>-3.454554</td>\n",
              "      <td>-4.884564</td>\n",
              "      <td>29.506064</td>\n",
              "      <td>-20.730002</td>\n",
              "      <td>-6.040676</td>\n",
              "      <td>190.427851</td>\n",
              "      <td>299.586559</td>\n",
              "      <td>-1.014003</td>\n",
              "      <td>0.260052</td>\n",
              "      <td>57.109047</td>\n",
              "      <td>192.539229</td>\n",
              "      <td>-1.097170</td>\n",
              "      <td>-0.087865</td>\n",
              "      <td>44.477327</td>\n",
              "      <td>191.057528</td>\n",
              "      <td>-1.188472</td>\n",
              "      <td>-0.022678</td>\n",
              "      <td>55.270113</td>\n",
              "      <td>212.522263</td>\n",
              "      <td>-1.142896</td>\n",
              "      <td>-0.167176</td>\n",
              "      <td>50.414646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.911566</td>\n",
              "      <td>0.942109</td>\n",
              "      <td>0.914162</td>\n",
              "      <td>0.834175</td>\n",
              "      <td>0.880634</td>\n",
              "      <td>0.801262</td>\n",
              "      <td>0.745575</td>\n",
              "      <td>0.866079</td>\n",
              "      <td>0.908687</td>\n",
              "      <td>0.113341</td>\n",
              "      <td>0.076214</td>\n",
              "      <td>0.086381</td>\n",
              "      <td>0.111883</td>\n",
              "      <td>0.166179</td>\n",
              "      <td>0.369518</td>\n",
              "      <td>0.535781</td>\n",
              "      <td>-0.255908</td>\n",
              "      <td>0.167986</td>\n",
              "      <td>0.680787</td>\n",
              "      <td>-0.218376</td>\n",
              "      <td>0.177517</td>\n",
              "      <td>0.778523</td>\n",
              "      <td>-0.130853</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>1</td>\n",
              "      <td>1.817</td>\n",
              "      <td>45.703</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59570.000</td>\n",
              "      <td>34</td>\n",
              "      <td>108.7</td>\n",
              "      <td>117.7</td>\n",
              "      <td>119.9</td>\n",
              "      <td>149.6</td>\n",
              "      <td>147.9</td>\n",
              "      <td>150.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>730</td>\n",
              "      <td>-19.159811</td>\n",
              "      <td>47.310059</td>\n",
              "      <td>2.267434</td>\n",
              "      <td>0.409172</td>\n",
              "      <td>8.022239</td>\n",
              "      <td>3.177854</td>\n",
              "      <td>0.695106</td>\n",
              "      <td>11.281384</td>\n",
              "      <td>2.471061</td>\n",
              "      <td>1.990851</td>\n",
              "      <td>1.721134</td>\n",
              "      <td>1.823726</td>\n",
              "      <td>0.069697</td>\n",
              "      <td>4.124452e+03</td>\n",
              "      <td>5.480405</td>\n",
              "      <td>1.046502e+05</td>\n",
              "      <td>5.989138</td>\n",
              "      <td>25.373110</td>\n",
              "      <td>66.469870</td>\n",
              "      <td>29.315018</td>\n",
              "      <td>2.619697</td>\n",
              "      <td>3.461790</td>\n",
              "      <td>4.729538</td>\n",
              "      <td>0.474215</td>\n",
              "      <td>0.356910</td>\n",
              "      <td>7.334944</td>\n",
              "      <td>13.515895</td>\n",
              "      <td>0.976374</td>\n",
              "      <td>0.471342</td>\n",
              "      <td>124.845250</td>\n",
              "      <td>119.500254</td>\n",
              "      <td>5.131290</td>\n",
              "      <td>2.385066</td>\n",
              "      <td>168.280524</td>\n",
              "      <td>162.799417</td>\n",
              "      <td>7.125665</td>\n",
              "      <td>2.662075</td>\n",
              "      <td>219.745132</td>\n",
              "      <td>...</td>\n",
              "      <td>0.947548</td>\n",
              "      <td>1.026690</td>\n",
              "      <td>0.959602</td>\n",
              "      <td>0.969243</td>\n",
              "      <td>0.903630</td>\n",
              "      <td>0.820510</td>\n",
              "      <td>0.868992</td>\n",
              "      <td>0.920047</td>\n",
              "      <td>0.935167</td>\n",
              "      <td>0.758565</td>\n",
              "      <td>0.657343</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.292954</td>\n",
              "      <td>0.318256</td>\n",
              "      <td>0.500549</td>\n",
              "      <td>-0.003923</td>\n",
              "      <td>0.211586</td>\n",
              "      <td>4.263530</td>\n",
              "      <td>5.710620</td>\n",
              "      <td>5.226490</td>\n",
              "      <td>-0.328019</td>\n",
              "      <td>0.481251</td>\n",
              "      <td>0.419737</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.233</td>\n",
              "      <td>40.328</td>\n",
              "      <td>0.004</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60444.379</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>745</td>\n",
              "      <td>-15.494463</td>\n",
              "      <td>220.795212</td>\n",
              "      <td>8.909206</td>\n",
              "      <td>1.035895</td>\n",
              "      <td>27.558208</td>\n",
              "      <td>4.979826</td>\n",
              "      <td>0.567170</td>\n",
              "      <td>55.892746</td>\n",
              "      <td>2.555576</td>\n",
              "      <td>1.819875</td>\n",
              "      <td>3.537324</td>\n",
              "      <td>10.741655</td>\n",
              "      <td>0.173789</td>\n",
              "      <td>9.416165e+04</td>\n",
              "      <td>9.611274</td>\n",
              "      <td>1.439125e+07</td>\n",
              "      <td>11.141069</td>\n",
              "      <td>152.835617</td>\n",
              "      <td>236.289675</td>\n",
              "      <td>26.521968</td>\n",
              "      <td>1.546038</td>\n",
              "      <td>129.421659</td>\n",
              "      <td>123.298327</td>\n",
              "      <td>4.629801</td>\n",
              "      <td>2.023211</td>\n",
              "      <td>320.174052</td>\n",
              "      <td>280.440312</td>\n",
              "      <td>50.868880</td>\n",
              "      <td>7.007099</td>\n",
              "      <td>543.845781</td>\n",
              "      <td>491.548270</td>\n",
              "      <td>36.088137</td>\n",
              "      <td>5.688194</td>\n",
              "      <td>807.123762</td>\n",
              "      <td>710.721942</td>\n",
              "      <td>16.392533</td>\n",
              "      <td>3.751603</td>\n",
              "      <td>735.528417</td>\n",
              "      <td>...</td>\n",
              "      <td>0.626753</td>\n",
              "      <td>0.787122</td>\n",
              "      <td>0.706331</td>\n",
              "      <td>0.602989</td>\n",
              "      <td>0.711901</td>\n",
              "      <td>0.444723</td>\n",
              "      <td>0.549658</td>\n",
              "      <td>0.728291</td>\n",
              "      <td>0.705632</td>\n",
              "      <td>0.432147</td>\n",
              "      <td>0.793649</td>\n",
              "      <td>0.472495</td>\n",
              "      <td>0.290652</td>\n",
              "      <td>0.360868</td>\n",
              "      <td>0.489589</td>\n",
              "      <td>-0.930442</td>\n",
              "      <td>-0.175522</td>\n",
              "      <td>3.695610</td>\n",
              "      <td>4.482710</td>\n",
              "      <td>8.055970</td>\n",
              "      <td>4.179630</td>\n",
              "      <td>0.488569</td>\n",
              "      <td>0.397251</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "      <td>0.301</td>\n",
              "      <td>40.969</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>257.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60130.453</td>\n",
              "      <td>38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1124</td>\n",
              "      <td>-16.543753</td>\n",
              "      <td>143.600189</td>\n",
              "      <td>7.145702</td>\n",
              "      <td>1.141288</td>\n",
              "      <td>20.051722</td>\n",
              "      <td>4.406298</td>\n",
              "      <td>0.695277</td>\n",
              "      <td>11.383690</td>\n",
              "      <td>2.753004</td>\n",
              "      <td>2.214854</td>\n",
              "      <td>1.933837</td>\n",
              "      <td>1.794938</td>\n",
              "      <td>0.173295</td>\n",
              "      <td>3.432418e+04</td>\n",
              "      <td>7.868462</td>\n",
              "      <td>3.015599e+06</td>\n",
              "      <td>7.908174</td>\n",
              "      <td>87.856390</td>\n",
              "      <td>160.143942</td>\n",
              "      <td>22.411225</td>\n",
              "      <td>1.822792</td>\n",
              "      <td>41.639721</td>\n",
              "      <td>32.987125</td>\n",
              "      <td>0.822496</td>\n",
              "      <td>-0.332169</td>\n",
              "      <td>268.808929</td>\n",
              "      <td>207.812015</td>\n",
              "      <td>6.112295</td>\n",
              "      <td>2.377222</td>\n",
              "      <td>594.150153</td>\n",
              "      <td>498.509820</td>\n",
              "      <td>10.343254</td>\n",
              "      <td>3.075437</td>\n",
              "      <td>643.020183</td>\n",
              "      <td>555.512641</td>\n",
              "      <td>14.095862</td>\n",
              "      <td>3.603208</td>\n",
              "      <td>574.553907</td>\n",
              "      <td>...</td>\n",
              "      <td>0.571537</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.968800</td>\n",
              "      <td>0.993363</td>\n",
              "      <td>0.612514</td>\n",
              "      <td>0.606935</td>\n",
              "      <td>0.551963</td>\n",
              "      <td>0.636599</td>\n",
              "      <td>0.997381</td>\n",
              "      <td>0.559273</td>\n",
              "      <td>0.122167</td>\n",
              "      <td>0.147048</td>\n",
              "      <td>0.250639</td>\n",
              "      <td>0.382847</td>\n",
              "      <td>0.673592</td>\n",
              "      <td>-0.021837</td>\n",
              "      <td>8.163350</td>\n",
              "      <td>21.595800</td>\n",
              "      <td>21.425300</td>\n",
              "      <td>15.357700</td>\n",
              "      <td>3.318960</td>\n",
              "      <td>0.590183</td>\n",
              "      <td>0.572176</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "      <td>0.193</td>\n",
              "      <td>39.866</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-368.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60452.641</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 372 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  object_id     flux_min  ...  tflux_i  tflux_z  tflux_y\n",
              "0           0        615 -1100.440063  ...   3039.7   2854.5   2837.0\n",
              "1           1        713   -14.735178  ...    149.6    147.9    150.5\n",
              "2           2        730   -19.159811  ...      0.0      0.0      0.0\n",
              "3           3        745   -15.494463  ...      0.0      0.0      0.0\n",
              "4           4       1124   -16.543753  ...      0.0      0.0      0.0\n",
              "\n",
              "[5 rows x 372 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLxnM5pmgqDe",
        "colab_type": "code",
        "outputId": "95e21bcd-1e9c-4673-8f16-3ecbeb25aad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.columns.get_loc(\"target\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VIsdkTShbUK",
        "colab_type": "code",
        "outputId": "c68c3e93-ed48-4159-d83c-e3b2e1b7934f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.columns.get_loc(\"true_target\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "356"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tu8FA7z-RnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = df.sample(frac=1)\n",
        "# df.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKZ7s5va-WHk",
        "colab_type": "code",
        "outputId": "47c6a0e0-cd1d-4dc5-ae3f-c906020ce282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "df.iloc[:1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>object_id</th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_ratio_sq_sum</th>\n",
              "      <th>flux_ratio_sq_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>flux_w_mean</th>\n",
              "      <th>flux_diff1</th>\n",
              "      <th>flux_diff2</th>\n",
              "      <th>flux_diff3</th>\n",
              "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>0__kurtosis</th>\n",
              "      <th>0__skewness</th>\n",
              "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>1__kurtosis</th>\n",
              "      <th>1__skewness</th>\n",
              "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>2__kurtosis</th>\n",
              "      <th>2__skewness</th>\n",
              "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>3__kurtosis</th>\n",
              "      <th>3__skewness</th>\n",
              "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>...</th>\n",
              "      <th>__freq_signif_ratio_21___3_</th>\n",
              "      <th>__freq_signif_ratio_21___4_</th>\n",
              "      <th>__freq_signif_ratio_21___5_</th>\n",
              "      <th>__freq_signif_ratio_31___0_</th>\n",
              "      <th>__freq_signif_ratio_31___1_</th>\n",
              "      <th>__freq_signif_ratio_31___2_</th>\n",
              "      <th>__freq_signif_ratio_31___3_</th>\n",
              "      <th>__freq_signif_ratio_31___4_</th>\n",
              "      <th>__freq_signif_ratio_31___5_</th>\n",
              "      <th>__freq_varrat___0_</th>\n",
              "      <th>__freq_varrat___1_</th>\n",
              "      <th>__freq_varrat___2_</th>\n",
              "      <th>__freq_varrat___3_</th>\n",
              "      <th>__freq_varrat___4_</th>\n",
              "      <th>__freq_varrat___5_</th>\n",
              "      <th>__freq_y_offset___0_</th>\n",
              "      <th>__freq_y_offset___1_</th>\n",
              "      <th>__freq_y_offset___2_</th>\n",
              "      <th>__freq_y_offset___3_</th>\n",
              "      <th>__freq_y_offset___4_</th>\n",
              "      <th>__freq_y_offset___5_</th>\n",
              "      <th>time_score</th>\n",
              "      <th>phase_score</th>\n",
              "      <th>ddf_bool</th>\n",
              "      <th>true_target</th>\n",
              "      <th>true_submodel</th>\n",
              "      <th>true_z</th>\n",
              "      <th>true_distmod</th>\n",
              "      <th>true_lensdmu</th>\n",
              "      <th>true_vpec</th>\n",
              "      <th>true_rv</th>\n",
              "      <th>true_av</th>\n",
              "      <th>true_peakmjd</th>\n",
              "      <th>libid_cadence</th>\n",
              "      <th>tflux_u</th>\n",
              "      <th>tflux_g</th>\n",
              "      <th>tflux_r</th>\n",
              "      <th>tflux_i</th>\n",
              "      <th>tflux_z</th>\n",
              "      <th>tflux_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>615</td>\n",
              "      <td>-1100.440063</td>\n",
              "      <td>660.626343</td>\n",
              "      <td>-123.096998</td>\n",
              "      <td>-89.477524</td>\n",
              "      <td>394.109851</td>\n",
              "      <td>-0.34954</td>\n",
              "      <td>2.13051</td>\n",
              "      <td>12.845472</td>\n",
              "      <td>4.482743</td>\n",
              "      <td>3.835269</td>\n",
              "      <td>1.744747</td>\n",
              "      <td>1.62374</td>\n",
              "      <td>0.946023</td>\n",
              "      <td>2.929669e+06</td>\n",
              "      <td>0.812722</td>\n",
              "      <td>-9.601766e+08</td>\n",
              "      <td>-1.414322</td>\n",
              "      <td>-327.742307</td>\n",
              "      <td>1761.066406</td>\n",
              "      <td>-14.306331</td>\n",
              "      <td>-5.373326</td>\n",
              "      <td>205.036926</td>\n",
              "      <td>1628.427737</td>\n",
              "      <td>-1.475181</td>\n",
              "      <td>0.128917</td>\n",
              "      <td>22370.594834</td>\n",
              "      <td>2806.374162</td>\n",
              "      <td>-1.255123</td>\n",
              "      <td>0.41558</td>\n",
              "      <td>7780.500807</td>\n",
              "      <td>2805.598113</td>\n",
              "      <td>-1.409885</td>\n",
              "      <td>0.339918</td>\n",
              "      <td>7024.003068</td>\n",
              "      <td>2536.068846</td>\n",
              "      <td>-1.449858</td>\n",
              "      <td>0.293128</td>\n",
              "      <td>3245.366349</td>\n",
              "      <td>...</td>\n",
              "      <td>0.919127</td>\n",
              "      <td>0.936241</td>\n",
              "      <td>0.980085</td>\n",
              "      <td>0.75043</td>\n",
              "      <td>0.798199</td>\n",
              "      <td>0.734428</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.628905</td>\n",
              "      <td>0.915124</td>\n",
              "      <td>0.12755</td>\n",
              "      <td>0.269031</td>\n",
              "      <td>0.139362</td>\n",
              "      <td>0.110785</td>\n",
              "      <td>0.129578</td>\n",
              "      <td>0.401664</td>\n",
              "      <td>4.17331</td>\n",
              "      <td>-29.9084</td>\n",
              "      <td>18.7479</td>\n",
              "      <td>17.0908</td>\n",
              "      <td>20.412</td>\n",
              "      <td>12.1033</td>\n",
              "      <td>0.197098</td>\n",
              "      <td>0.934001</td>\n",
              "      <td>1</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59570.0</td>\n",
              "      <td>69</td>\n",
              "      <td>484.7</td>\n",
              "      <td>3286.7</td>\n",
              "      <td>3214.1</td>\n",
              "      <td>3039.7</td>\n",
              "      <td>2854.5</td>\n",
              "      <td>2837.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 372 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  object_id     flux_min  ...  tflux_i  tflux_z  tflux_y\n",
              "0           0        615 -1100.440063  ...   3039.7   2854.5   2837.0\n",
              "\n",
              "[1 rows x 372 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9YQkypyZiyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ =np.nan_to_num(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcoY1qsIZzms",
        "colab_type": "code",
        "outputId": "56396a93-61f2-44a6-bb62-880023ed8a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.any(np.isinf(df_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8zKQstcfprD",
        "colab_type": "code",
        "outputId": "78da3403-4b52-4e65-8619-272d29b57982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(df_)\n",
        "df_ = pd.DataFrame(x_scaled)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWDYUI_VvCFG",
        "colab_type": "code",
        "outputId": "4498d8f3-825d-47b8-cf0d-1ec6f498173c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "df_.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>332</th>\n",
              "      <th>333</th>\n",
              "      <th>334</th>\n",
              "      <th>335</th>\n",
              "      <th>336</th>\n",
              "      <th>337</th>\n",
              "      <th>338</th>\n",
              "      <th>339</th>\n",
              "      <th>340</th>\n",
              "      <th>341</th>\n",
              "      <th>342</th>\n",
              "      <th>343</th>\n",
              "      <th>344</th>\n",
              "      <th>345</th>\n",
              "      <th>346</th>\n",
              "      <th>347</th>\n",
              "      <th>348</th>\n",
              "      <th>349</th>\n",
              "      <th>350</th>\n",
              "      <th>351</th>\n",
              "      <th>352</th>\n",
              "      <th>353</th>\n",
              "      <th>354</th>\n",
              "      <th>355</th>\n",
              "      <th>356</th>\n",
              "      <th>357</th>\n",
              "      <th>358</th>\n",
              "      <th>359</th>\n",
              "      <th>360</th>\n",
              "      <th>361</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "      <th>369</th>\n",
              "      <th>370</th>\n",
              "      <th>371</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.998599</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.253477</td>\n",
              "      <td>0.582881</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.398073</td>\n",
              "      <td>0.005095</td>\n",
              "      <td>1.692928e-06</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.231565</td>\n",
              "      <td>0.945714</td>\n",
              "      <td>0.269740</td>\n",
              "      <td>0.102732</td>\n",
              "      <td>0.419864</td>\n",
              "      <td>0.457157</td>\n",
              "      <td>0.279084</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.522296</td>\n",
              "      <td>0.539588</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>3.515728e-04</td>\n",
              "      <td>0.023115</td>\n",
              "      <td>0.486666</td>\n",
              "      <td>0.035217</td>\n",
              "      <td>0.006315</td>\n",
              "      <td>0.048923</td>\n",
              "      <td>0.520752</td>\n",
              "      <td>0.002080</td>\n",
              "      <td>0.001155</td>\n",
              "      <td>0.016812</td>\n",
              "      <td>0.522493</td>\n",
              "      <td>0.002151</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.011624</td>\n",
              "      <td>0.514756</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>...</td>\n",
              "      <td>0.631785</td>\n",
              "      <td>0.597880</td>\n",
              "      <td>0.626320</td>\n",
              "      <td>0.418006</td>\n",
              "      <td>0.337524</td>\n",
              "      <td>0.424850</td>\n",
              "      <td>0.434345</td>\n",
              "      <td>0.278001</td>\n",
              "      <td>0.539139</td>\n",
              "      <td>0.127557</td>\n",
              "      <td>0.268992</td>\n",
              "      <td>0.139321</td>\n",
              "      <td>0.110750</td>\n",
              "      <td>0.129510</td>\n",
              "      <td>0.401857</td>\n",
              "      <td>0.273375</td>\n",
              "      <td>0.396428</td>\n",
              "      <td>0.999929</td>\n",
              "      <td>0.496892</td>\n",
              "      <td>0.609820</td>\n",
              "      <td>0.492746</td>\n",
              "      <td>0.437672</td>\n",
              "      <td>0.959545</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.966292</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.793282</td>\n",
              "      <td>0.481390</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.453353</td>\n",
              "      <td>0.00138</td>\n",
              "      <td>0.016690</td>\n",
              "      <td>0.020350</td>\n",
              "      <td>0.017798</td>\n",
              "      <td>0.020112</td>\n",
              "      <td>0.017101</td>\n",
              "      <td>0.015200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000127</td>\n",
              "      <td>7.493545e-07</td>\n",
              "      <td>0.999543</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.254123</td>\n",
              "      <td>0.584075</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.409715</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>2.344565e-08</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.232165</td>\n",
              "      <td>0.166694</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.234246</td>\n",
              "      <td>0.423389</td>\n",
              "      <td>0.402260</td>\n",
              "      <td>0.280379</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.522246</td>\n",
              "      <td>0.539480</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>6.464922e-05</td>\n",
              "      <td>0.029284</td>\n",
              "      <td>0.494760</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.051468</td>\n",
              "      <td>0.487141</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.020477</td>\n",
              "      <td>0.498674</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.016752</td>\n",
              "      <td>0.484139</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>...</td>\n",
              "      <td>0.624041</td>\n",
              "      <td>0.604519</td>\n",
              "      <td>0.530395</td>\n",
              "      <td>0.464654</td>\n",
              "      <td>0.392311</td>\n",
              "      <td>0.484569</td>\n",
              "      <td>0.449170</td>\n",
              "      <td>0.554486</td>\n",
              "      <td>0.529920</td>\n",
              "      <td>0.113348</td>\n",
              "      <td>0.076164</td>\n",
              "      <td>0.086336</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.166125</td>\n",
              "      <td>0.369672</td>\n",
              "      <td>0.228406</td>\n",
              "      <td>0.476676</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.463559</td>\n",
              "      <td>0.552805</td>\n",
              "      <td>0.438561</td>\n",
              "      <td>0.851534</td>\n",
              "      <td>0.182289</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.921348</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.527737</td>\n",
              "      <td>0.964483</td>\n",
              "      <td>0.793282</td>\n",
              "      <td>0.481390</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.453353</td>\n",
              "      <td>0.00068</td>\n",
              "      <td>0.003743</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000255</td>\n",
              "      <td>8.793446e-07</td>\n",
              "      <td>0.999539</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>0.254143</td>\n",
              "      <td>0.584092</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.510730</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>9.928179e-07</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.244197</td>\n",
              "      <td>0.064381</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.336435</td>\n",
              "      <td>0.423389</td>\n",
              "      <td>0.656362</td>\n",
              "      <td>0.280500</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.522639</td>\n",
              "      <td>0.540883</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>9.836503e-07</td>\n",
              "      <td>0.049192</td>\n",
              "      <td>0.500738</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.084877</td>\n",
              "      <td>0.524475</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.125095</td>\n",
              "      <td>0.656837</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.154875</td>\n",
              "      <td>0.672325</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.660896</td>\n",
              "      <td>0.700214</td>\n",
              "      <td>0.596515</td>\n",
              "      <td>0.539890</td>\n",
              "      <td>0.407594</td>\n",
              "      <td>0.501768</td>\n",
              "      <td>0.579188</td>\n",
              "      <td>0.617399</td>\n",
              "      <td>0.567847</td>\n",
              "      <td>0.758609</td>\n",
              "      <td>0.657327</td>\n",
              "      <td>0.204572</td>\n",
              "      <td>0.292940</td>\n",
              "      <td>0.318262</td>\n",
              "      <td>0.500862</td>\n",
              "      <td>0.221734</td>\n",
              "      <td>0.477941</td>\n",
              "      <td>0.999924</td>\n",
              "      <td>0.473776</td>\n",
              "      <td>0.567853</td>\n",
              "      <td>0.436264</td>\n",
              "      <td>0.639934</td>\n",
              "      <td>0.584175</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.404494</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.067674</td>\n",
              "      <td>0.851053</td>\n",
              "      <td>0.803618</td>\n",
              "      <td>0.483366</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.747074</td>\n",
              "      <td>0.00018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000382</td>\n",
              "      <td>9.940417e-07</td>\n",
              "      <td>0.999542</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.254178</td>\n",
              "      <td>0.584101</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.568282</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>2.096156e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.807492</td>\n",
              "      <td>0.169068</td>\n",
              "      <td>0.008658</td>\n",
              "      <td>0.543261</td>\n",
              "      <td>0.423442</td>\n",
              "      <td>0.794985</td>\n",
              "      <td>0.281012</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.522617</td>\n",
              "      <td>0.540709</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>2.658504e-05</td>\n",
              "      <td>0.104780</td>\n",
              "      <td>0.603580</td>\n",
              "      <td>0.000504</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.888741</td>\n",
              "      <td>0.960817</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>0.873816</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.309674</td>\n",
              "      <td>0.744794</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>...</td>\n",
              "      <td>0.332318</td>\n",
              "      <td>0.429168</td>\n",
              "      <td>0.227978</td>\n",
              "      <td>0.335878</td>\n",
              "      <td>0.280169</td>\n",
              "      <td>0.165986</td>\n",
              "      <td>0.242775</td>\n",
              "      <td>0.393860</td>\n",
              "      <td>0.239087</td>\n",
              "      <td>0.432172</td>\n",
              "      <td>0.793641</td>\n",
              "      <td>0.472483</td>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.360891</td>\n",
              "      <td>0.489889</td>\n",
              "      <td>0.210280</td>\n",
              "      <td>0.476894</td>\n",
              "      <td>0.999923</td>\n",
              "      <td>0.471281</td>\n",
              "      <td>0.575672</td>\n",
              "      <td>0.456745</td>\n",
              "      <td>0.645143</td>\n",
              "      <td>0.567762</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.943820</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.087424</td>\n",
              "      <td>0.864580</td>\n",
              "      <td>0.782946</td>\n",
              "      <td>0.594496</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.641620</td>\n",
              "      <td>0.00076</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000510</td>\n",
              "      <td>3.892056e-06</td>\n",
              "      <td>0.999541</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.254169</td>\n",
              "      <td>0.584102</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.549964</td>\n",
              "      <td>0.000708</td>\n",
              "      <td>1.038612e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.242378</td>\n",
              "      <td>0.168571</td>\n",
              "      <td>0.003149</td>\n",
              "      <td>0.456001</td>\n",
              "      <td>0.423400</td>\n",
              "      <td>0.707997</td>\n",
              "      <td>0.280751</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.522585</td>\n",
              "      <td>0.540754</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>7.085033e-06</td>\n",
              "      <td>0.053851</td>\n",
              "      <td>0.458209</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.167627</td>\n",
              "      <td>0.651716</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.211374</td>\n",
              "      <td>0.702187</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.271309</td>\n",
              "      <td>0.734923</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.275762</td>\n",
              "      <td>0.237176</td>\n",
              "      <td>0.609899</td>\n",
              "      <td>0.553325</td>\n",
              "      <td>0.214116</td>\n",
              "      <td>0.310929</td>\n",
              "      <td>0.245203</td>\n",
              "      <td>0.286970</td>\n",
              "      <td>0.656955</td>\n",
              "      <td>0.559305</td>\n",
              "      <td>0.122120</td>\n",
              "      <td>0.147008</td>\n",
              "      <td>0.250620</td>\n",
              "      <td>0.382879</td>\n",
              "      <td>0.674116</td>\n",
              "      <td>0.221512</td>\n",
              "      <td>0.499461</td>\n",
              "      <td>0.999930</td>\n",
              "      <td>0.505696</td>\n",
              "      <td>0.595852</td>\n",
              "      <td>0.452834</td>\n",
              "      <td>0.717472</td>\n",
              "      <td>0.695442</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.943820</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.056056</td>\n",
              "      <td>0.841303</td>\n",
              "      <td>0.788114</td>\n",
              "      <td>0.319522</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.749849</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 372 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0             1         2    ...       369       370       371\n",
              "0  0.000000  0.000000e+00  0.998599  ...  0.020112  0.017101  0.015200\n",
              "1  0.000127  7.493545e-07  0.999543  ...  0.000990  0.000886  0.000806\n",
              "2  0.000255  8.793446e-07  0.999539  ...  0.000000  0.000000  0.000000\n",
              "3  0.000382  9.940417e-07  0.999542  ...  0.000000  0.000000  0.000000\n",
              "4  0.000510  3.892056e-06  0.999541  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[5 rows x 372 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szp6dk7U3yfB",
        "colab_type": "code",
        "outputId": "c8f9c2ea-e8fb-4417-f9c6-9fc99cc002e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "df.iloc[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>object_id</th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_ratio_sq_sum</th>\n",
              "      <th>flux_ratio_sq_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>flux_w_mean</th>\n",
              "      <th>flux_diff1</th>\n",
              "      <th>flux_diff2</th>\n",
              "      <th>flux_diff3</th>\n",
              "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>0__kurtosis</th>\n",
              "      <th>0__skewness</th>\n",
              "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>1__kurtosis</th>\n",
              "      <th>1__skewness</th>\n",
              "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>2__kurtosis</th>\n",
              "      <th>2__skewness</th>\n",
              "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>3__kurtosis</th>\n",
              "      <th>3__skewness</th>\n",
              "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>...</th>\n",
              "      <th>__freq_signif_ratio_21___3_</th>\n",
              "      <th>__freq_signif_ratio_21___4_</th>\n",
              "      <th>__freq_signif_ratio_21___5_</th>\n",
              "      <th>__freq_signif_ratio_31___0_</th>\n",
              "      <th>__freq_signif_ratio_31___1_</th>\n",
              "      <th>__freq_signif_ratio_31___2_</th>\n",
              "      <th>__freq_signif_ratio_31___3_</th>\n",
              "      <th>__freq_signif_ratio_31___4_</th>\n",
              "      <th>__freq_signif_ratio_31___5_</th>\n",
              "      <th>__freq_varrat___0_</th>\n",
              "      <th>__freq_varrat___1_</th>\n",
              "      <th>__freq_varrat___2_</th>\n",
              "      <th>__freq_varrat___3_</th>\n",
              "      <th>__freq_varrat___4_</th>\n",
              "      <th>__freq_varrat___5_</th>\n",
              "      <th>__freq_y_offset___0_</th>\n",
              "      <th>__freq_y_offset___1_</th>\n",
              "      <th>__freq_y_offset___2_</th>\n",
              "      <th>__freq_y_offset___3_</th>\n",
              "      <th>__freq_y_offset___4_</th>\n",
              "      <th>__freq_y_offset___5_</th>\n",
              "      <th>time_score</th>\n",
              "      <th>phase_score</th>\n",
              "      <th>ddf_bool</th>\n",
              "      <th>true_target</th>\n",
              "      <th>true_submodel</th>\n",
              "      <th>true_z</th>\n",
              "      <th>true_distmod</th>\n",
              "      <th>true_lensdmu</th>\n",
              "      <th>true_vpec</th>\n",
              "      <th>true_rv</th>\n",
              "      <th>true_av</th>\n",
              "      <th>true_peakmjd</th>\n",
              "      <th>libid_cadence</th>\n",
              "      <th>tflux_u</th>\n",
              "      <th>tflux_g</th>\n",
              "      <th>tflux_r</th>\n",
              "      <th>tflux_i</th>\n",
              "      <th>tflux_z</th>\n",
              "      <th>tflux_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>615</td>\n",
              "      <td>-1100.440063</td>\n",
              "      <td>660.626343</td>\n",
              "      <td>-123.096998</td>\n",
              "      <td>-89.477524</td>\n",
              "      <td>394.109851</td>\n",
              "      <td>-0.349540</td>\n",
              "      <td>2.130510</td>\n",
              "      <td>12.845472</td>\n",
              "      <td>4.482743</td>\n",
              "      <td>3.835269</td>\n",
              "      <td>1.744747</td>\n",
              "      <td>1.623740</td>\n",
              "      <td>0.946023</td>\n",
              "      <td>2.929669e+06</td>\n",
              "      <td>0.812722</td>\n",
              "      <td>-9.601766e+08</td>\n",
              "      <td>-1.414322</td>\n",
              "      <td>-327.742307</td>\n",
              "      <td>1761.066406</td>\n",
              "      <td>-14.306331</td>\n",
              "      <td>-5.373326</td>\n",
              "      <td>205.036926</td>\n",
              "      <td>1628.427737</td>\n",
              "      <td>-1.475181</td>\n",
              "      <td>0.128917</td>\n",
              "      <td>22370.594834</td>\n",
              "      <td>2806.374162</td>\n",
              "      <td>-1.255123</td>\n",
              "      <td>0.415580</td>\n",
              "      <td>7780.500807</td>\n",
              "      <td>2805.598113</td>\n",
              "      <td>-1.409885</td>\n",
              "      <td>0.339918</td>\n",
              "      <td>7024.003068</td>\n",
              "      <td>2536.068846</td>\n",
              "      <td>-1.449858</td>\n",
              "      <td>0.293128</td>\n",
              "      <td>3245.366349</td>\n",
              "      <td>...</td>\n",
              "      <td>0.919127</td>\n",
              "      <td>0.936241</td>\n",
              "      <td>0.980085</td>\n",
              "      <td>0.750430</td>\n",
              "      <td>0.798199</td>\n",
              "      <td>0.734428</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.628905</td>\n",
              "      <td>0.915124</td>\n",
              "      <td>0.127550</td>\n",
              "      <td>0.269031</td>\n",
              "      <td>0.139362</td>\n",
              "      <td>0.110785</td>\n",
              "      <td>0.129578</td>\n",
              "      <td>0.401664</td>\n",
              "      <td>4.173310</td>\n",
              "      <td>-29.908400</td>\n",
              "      <td>18.747900</td>\n",
              "      <td>17.090800</td>\n",
              "      <td>20.412000</td>\n",
              "      <td>12.103300</td>\n",
              "      <td>0.197098</td>\n",
              "      <td>0.934001</td>\n",
              "      <td>1</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59570.000</td>\n",
              "      <td>69</td>\n",
              "      <td>484.7</td>\n",
              "      <td>3286.7</td>\n",
              "      <td>3214.1</td>\n",
              "      <td>3039.7</td>\n",
              "      <td>2854.5</td>\n",
              "      <td>2837.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>713</td>\n",
              "      <td>-14.735178</td>\n",
              "      <td>14.770886</td>\n",
              "      <td>-1.423351</td>\n",
              "      <td>-0.873033</td>\n",
              "      <td>6.471144</td>\n",
              "      <td>0.014989</td>\n",
              "      <td>0.639458</td>\n",
              "      <td>9.115748</td>\n",
              "      <td>2.359620</td>\n",
              "      <td>1.998217</td>\n",
              "      <td>1.509888</td>\n",
              "      <td>1.633246</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>5.886068e+03</td>\n",
              "      <td>3.439423</td>\n",
              "      <td>-2.875087e+04</td>\n",
              "      <td>-3.454554</td>\n",
              "      <td>-4.884564</td>\n",
              "      <td>29.506064</td>\n",
              "      <td>-20.730002</td>\n",
              "      <td>-6.040676</td>\n",
              "      <td>190.427851</td>\n",
              "      <td>299.586559</td>\n",
              "      <td>-1.014003</td>\n",
              "      <td>0.260052</td>\n",
              "      <td>57.109047</td>\n",
              "      <td>192.539229</td>\n",
              "      <td>-1.097170</td>\n",
              "      <td>-0.087865</td>\n",
              "      <td>44.477327</td>\n",
              "      <td>191.057528</td>\n",
              "      <td>-1.188472</td>\n",
              "      <td>-0.022678</td>\n",
              "      <td>55.270113</td>\n",
              "      <td>212.522263</td>\n",
              "      <td>-1.142896</td>\n",
              "      <td>-0.167176</td>\n",
              "      <td>50.414646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.911566</td>\n",
              "      <td>0.942109</td>\n",
              "      <td>0.914162</td>\n",
              "      <td>0.834175</td>\n",
              "      <td>0.880634</td>\n",
              "      <td>0.801262</td>\n",
              "      <td>0.745575</td>\n",
              "      <td>0.866079</td>\n",
              "      <td>0.908687</td>\n",
              "      <td>0.113341</td>\n",
              "      <td>0.076214</td>\n",
              "      <td>0.086381</td>\n",
              "      <td>0.111883</td>\n",
              "      <td>0.166179</td>\n",
              "      <td>0.369518</td>\n",
              "      <td>0.535781</td>\n",
              "      <td>-0.255908</td>\n",
              "      <td>0.167986</td>\n",
              "      <td>0.680787</td>\n",
              "      <td>-0.218376</td>\n",
              "      <td>0.177517</td>\n",
              "      <td>0.778523</td>\n",
              "      <td>-0.130853</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>1</td>\n",
              "      <td>1.817</td>\n",
              "      <td>45.703</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59570.000</td>\n",
              "      <td>34</td>\n",
              "      <td>108.7</td>\n",
              "      <td>117.7</td>\n",
              "      <td>119.9</td>\n",
              "      <td>149.6</td>\n",
              "      <td>147.9</td>\n",
              "      <td>150.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>730</td>\n",
              "      <td>-19.159811</td>\n",
              "      <td>47.310059</td>\n",
              "      <td>2.267434</td>\n",
              "      <td>0.409172</td>\n",
              "      <td>8.022239</td>\n",
              "      <td>3.177854</td>\n",
              "      <td>0.695106</td>\n",
              "      <td>11.281384</td>\n",
              "      <td>2.471061</td>\n",
              "      <td>1.990851</td>\n",
              "      <td>1.721134</td>\n",
              "      <td>1.823726</td>\n",
              "      <td>0.069697</td>\n",
              "      <td>4.124452e+03</td>\n",
              "      <td>5.480405</td>\n",
              "      <td>1.046502e+05</td>\n",
              "      <td>5.989138</td>\n",
              "      <td>25.373110</td>\n",
              "      <td>66.469870</td>\n",
              "      <td>29.315018</td>\n",
              "      <td>2.619697</td>\n",
              "      <td>3.461790</td>\n",
              "      <td>4.729538</td>\n",
              "      <td>0.474215</td>\n",
              "      <td>0.356910</td>\n",
              "      <td>7.334944</td>\n",
              "      <td>13.515895</td>\n",
              "      <td>0.976374</td>\n",
              "      <td>0.471342</td>\n",
              "      <td>124.845250</td>\n",
              "      <td>119.500254</td>\n",
              "      <td>5.131290</td>\n",
              "      <td>2.385066</td>\n",
              "      <td>168.280524</td>\n",
              "      <td>162.799417</td>\n",
              "      <td>7.125665</td>\n",
              "      <td>2.662075</td>\n",
              "      <td>219.745132</td>\n",
              "      <td>...</td>\n",
              "      <td>0.947548</td>\n",
              "      <td>1.026690</td>\n",
              "      <td>0.959602</td>\n",
              "      <td>0.969243</td>\n",
              "      <td>0.903630</td>\n",
              "      <td>0.820510</td>\n",
              "      <td>0.868992</td>\n",
              "      <td>0.920047</td>\n",
              "      <td>0.935167</td>\n",
              "      <td>0.758565</td>\n",
              "      <td>0.657343</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.292954</td>\n",
              "      <td>0.318256</td>\n",
              "      <td>0.500549</td>\n",
              "      <td>-0.003923</td>\n",
              "      <td>0.211586</td>\n",
              "      <td>4.263530</td>\n",
              "      <td>5.710620</td>\n",
              "      <td>5.226490</td>\n",
              "      <td>-0.328019</td>\n",
              "      <td>0.481251</td>\n",
              "      <td>0.419737</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.233</td>\n",
              "      <td>40.328</td>\n",
              "      <td>0.004</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60444.379</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 372 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  object_id     flux_min  ...  tflux_i  tflux_z  tflux_y\n",
              "0           0        615 -1100.440063  ...   3039.7   2854.5   2837.0\n",
              "1           1        713   -14.735178  ...    149.6    147.9    150.5\n",
              "2           2        730   -19.159811  ...      0.0      0.0      0.0\n",
              "\n",
              "[3 rows x 372 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfDVyPU7PdXb",
        "colab_type": "code",
        "outputId": "0a0bf07b-4266-4346-93b4-62591ab91ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "targets = df['target']\n",
        "classes = targets.unique()\n",
        "classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([92, 88, 42, 90, 65, 16, 67, 95, 62, 15, 52,  6, 64, 53])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PP3KyEyAsep",
        "colab_type": "code",
        "outputId": "6d92f550-17fb-4039-eff0-519ef826303d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "targets[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    92\n",
              "1    88\n",
              "2    42\n",
              "3    90\n",
              "4    90\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH5iMXIVXFoi",
        "colab_type": "code",
        "outputId": "ae594b33-bac0-490e-c4e7-2ea2f9a9248e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "class_map =[] \n",
        "\n",
        "for i,val in enumerate(classes):\n",
        "    class_map.append([val , i])\n",
        "class_map"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[92, 0],\n",
              " [88, 1],\n",
              " [42, 2],\n",
              " [90, 3],\n",
              " [65, 4],\n",
              " [16, 5],\n",
              " [67, 6],\n",
              " [95, 7],\n",
              " [62, 8],\n",
              " [15, 9],\n",
              " [52, 10],\n",
              " [6, 11],\n",
              " [64, 12],\n",
              " [53, 13]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oznQZGVqDTo",
        "colab_type": "code",
        "outputId": "d2e1b9e3-a420-446d-f854-8d01bb9a1417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "[class_map[i][0] for i in range(len(class_map))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[92, 88, 42, 90, 65, 16, 67, 95, 62, 15, 52, 6, 64, 53]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFMRg8wYj6yq",
        "colab_type": "code",
        "outputId": "feba749a-f9a6-45eb-eba5-79502d35a314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(class_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jshn4w9ZGxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(targets , class_map):\n",
        "    \n",
        "    vecs = []\n",
        "    \n",
        "    for j in targets:\n",
        "        \n",
        "        vec = np.zeros((classes.shape[0],))\n",
        "        \n",
        "        for i in range(len(class_map)):\n",
        "            if class_map[i][0]== j:\n",
        "                vec[class_map[i][1]] =+ 1\n",
        "                vecs.append(vec)\n",
        "    return vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utxkjtyc_Cev",
        "colab_type": "code",
        "outputId": "b2b25414-ab86-4122-aab9-486ec3129d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "one_hot(targets , class_map=class_map)[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ6gC9ekAt7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_structure(x):\n",
        "    return np.array(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwiD0Hx82-_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n_train = 3*df.shape[0]//4\n",
        "\n",
        "# X_train = df_.iloc[:n_train].drop([356, 67] ,1)\n",
        "# y_train = one_hot(targets , class_map=class_map)[:n_train]\n",
        "# # y_train = df['vectors'][:n_train]\n",
        "\n",
        "# X_test = df_.iloc[n_train:].drop([356, 67] ,1)\n",
        "# # y_test = df['vectors'][n_train:]\n",
        "# y_test = one_hot(targets , class_map=class_map)[n_train:]\n",
        "\n",
        "\n",
        "# X_train = remove_structure(X_train)\n",
        "# X_test = remove_structure(X_test)\n",
        "# y_train = remove_structure(y_train)\n",
        "# y_test = remove_structure(y_test)\n",
        "\n",
        "# input_dim = X_train.shape[1]\n",
        "# input_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27VVFZtsH3Eu",
        "colab_type": "code",
        "outputId": "ec7e415a-42e0-41f2-e9b3-fa99855a739b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7848, 372)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JI9LQKfAvlT",
        "colab_type": "code",
        "outputId": "b60bc9d4-439c-4d18-feb4-aa89c4f80890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = df_.drop([356, 67] ,1)\n",
        "y = one_hot(targets , class_map=class_map)\n",
        "\n",
        "X = remove_structure(X)\n",
        "y = remove_structure(y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "input_dim = X_train.shape[1]\n",
        "input_dim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "370"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFNNl6i4Pcjx",
        "colab_type": "code",
        "outputId": "338514bc-d490-4a42-9248-034dd92528fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5886, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzxSqQM6ByD2",
        "colab_type": "code",
        "outputId": "ccc5fd24-cad7-4475-ae05-b71eccca3727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5886, 370)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh3csHPKBbXi",
        "colab_type": "code",
        "outputId": "2ebad8d8-62d8-4c9c-e762-c6f3762ee1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_class = len(classes)\n",
        "n_class"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlCLpaLp19W2",
        "colab_type": "code",
        "outputId": "8cd16ca4-36b6-472a-976d-0ee2659455af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "af = 'relu'\n",
        "\n",
        "K.backend.clear_session()\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(input_dim, input_dim=input_dim))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "model.add(Dropout(rate = 0.3))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "model.add(Dropout(rate = 0.5))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "model.add(Dropout(rate = 0.3))\n",
        "\n",
        "model.add(Dense(32))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "\n",
        "model.add(Dense(32))\n",
        "model.add(BatchNormalization(trainable=True))\n",
        "model.add(Activation(af))\n",
        "model.add(Dropout(rate = 0.5))\n",
        "\n",
        "model.add(Dense(n_class, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0816 16:25:35.023186 139932106307456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "W0816 16:25:35.025567 139932106307456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0816 16:25:35.052879 139932106307456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0816 16:25:35.055164 139932106307456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0816 16:25:35.060679 139932106307456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0816 16:25:35.303508 139932106307456 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw6Mj-DeEqi3",
        "colab_type": "code",
        "outputId": "59a8efe2-d9ae-4a19-b21d-c63f4eeb4965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 370)               137270    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 370)               1480      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 370)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               94976     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 14)                462       \n",
            "=================================================================\n",
            "Total params: 301,964\n",
            "Trainable params: 299,816\n",
            "Non-trainable params: 2,148\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iZ4LB691gN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sgd = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "adam = K.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-3, amsgrad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0_HmpzF2RyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = K.losses.categorical_crossentropy, optimizer=adam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5itlbHP2nEq",
        "colab_type": "code",
        "outputId": "3297a78b-f8fa-47fc-8786-2d1ccc79bce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,y_train,epochs=100,batch_size=200,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5886/5886 [==============================] - 3s 463us/step - loss: 3.1458\n",
            "Epoch 2/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 3.0199\n",
            "Epoch 3/100\n",
            "5886/5886 [==============================] - 1s 128us/step - loss: 2.9788\n",
            "Epoch 4/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 2.8708\n",
            "Epoch 5/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 2.7875\n",
            "Epoch 6/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 2.7126\n",
            "Epoch 7/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 2.6270\n",
            "Epoch 8/100\n",
            "5886/5886 [==============================] - 1s 127us/step - loss: 2.5386\n",
            "Epoch 9/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 2.5048\n",
            "Epoch 10/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 2.4667\n",
            "Epoch 11/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 2.4196\n",
            "Epoch 12/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 2.3804\n",
            "Epoch 13/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 2.3545\n",
            "Epoch 14/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 2.3310\n",
            "Epoch 15/100\n",
            "5886/5886 [==============================] - 1s 127us/step - loss: 2.2914\n",
            "Epoch 16/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 2.2588\n",
            "Epoch 17/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 2.2490\n",
            "Epoch 18/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 2.2435\n",
            "Epoch 19/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 2.2116\n",
            "Epoch 20/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 2.1892\n",
            "Epoch 21/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 2.1744\n",
            "Epoch 22/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 2.1555\n",
            "Epoch 23/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 2.1285\n",
            "Epoch 24/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 2.1211\n",
            "Epoch 25/100\n",
            "5886/5886 [==============================] - 1s 124us/step - loss: 2.1266\n",
            "Epoch 26/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 2.0933\n",
            "Epoch 27/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 2.0818\n",
            "Epoch 28/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 2.0834\n",
            "Epoch 29/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 2.0672\n",
            "Epoch 30/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 2.0458\n",
            "Epoch 31/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 2.0428\n",
            "Epoch 32/100\n",
            "5886/5886 [==============================] - 1s 128us/step - loss: 2.0243\n",
            "Epoch 33/100\n",
            "5886/5886 [==============================] - 1s 118us/step - loss: 2.0130\n",
            "Epoch 34/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 2.0055\n",
            "Epoch 35/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 2.0019\n",
            "Epoch 36/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.9957\n",
            "Epoch 37/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 1.9513\n",
            "Epoch 38/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 1.9612\n",
            "Epoch 39/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.9468\n",
            "Epoch 40/100\n",
            "5886/5886 [==============================] - 1s 118us/step - loss: 1.9367\n",
            "Epoch 41/100\n",
            "5886/5886 [==============================] - 1s 116us/step - loss: 1.9194\n",
            "Epoch 42/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.9114\n",
            "Epoch 43/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.9058\n",
            "Epoch 44/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 1.8968\n",
            "Epoch 45/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.8792\n",
            "Epoch 46/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.8758\n",
            "Epoch 47/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 1.8673\n",
            "Epoch 48/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.8515\n",
            "Epoch 49/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.8359\n",
            "Epoch 50/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.8395\n",
            "Epoch 51/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.8098\n",
            "Epoch 52/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.8096\n",
            "Epoch 53/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.7972\n",
            "Epoch 54/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 1.7866\n",
            "Epoch 55/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.7840\n",
            "Epoch 56/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.7767\n",
            "Epoch 57/100\n",
            "5886/5886 [==============================] - 1s 118us/step - loss: 1.7582\n",
            "Epoch 58/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.7497\n",
            "Epoch 59/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.7305\n",
            "Epoch 60/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.7345\n",
            "Epoch 61/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.7244\n",
            "Epoch 62/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 1.7071\n",
            "Epoch 63/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.7009\n",
            "Epoch 64/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.7026\n",
            "Epoch 65/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.6753\n",
            "Epoch 66/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.6718\n",
            "Epoch 67/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.6636\n",
            "Epoch 68/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.6665\n",
            "Epoch 69/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.6480\n",
            "Epoch 70/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.6384\n",
            "Epoch 71/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 1.6499\n",
            "Epoch 72/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.6368\n",
            "Epoch 73/100\n",
            "5886/5886 [==============================] - 1s 125us/step - loss: 1.6299\n",
            "Epoch 74/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.6213\n",
            "Epoch 75/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.6084\n",
            "Epoch 76/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 1.5915\n",
            "Epoch 77/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 1.5912\n",
            "Epoch 78/100\n",
            "5886/5886 [==============================] - 1s 114us/step - loss: 1.6006\n",
            "Epoch 79/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.5763\n",
            "Epoch 80/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.5810\n",
            "Epoch 81/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.5670\n",
            "Epoch 82/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.5549\n",
            "Epoch 83/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 1.5472\n",
            "Epoch 84/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 1.5450\n",
            "Epoch 85/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.5393\n",
            "Epoch 86/100\n",
            "5886/5886 [==============================] - 1s 124us/step - loss: 1.5360\n",
            "Epoch 87/100\n",
            "5886/5886 [==============================] - 1s 118us/step - loss: 1.5271\n",
            "Epoch 88/100\n",
            "5886/5886 [==============================] - 1s 123us/step - loss: 1.5350\n",
            "Epoch 89/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.5216\n",
            "Epoch 90/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 1.5118\n",
            "Epoch 91/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.5107\n",
            "Epoch 92/100\n",
            "5886/5886 [==============================] - 1s 124us/step - loss: 1.5039\n",
            "Epoch 93/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 1.5053\n",
            "Epoch 94/100\n",
            "5886/5886 [==============================] - 1s 124us/step - loss: 1.4938\n",
            "Epoch 95/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.4879\n",
            "Epoch 96/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.4841\n",
            "Epoch 97/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.4833\n",
            "Epoch 98/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.4711\n",
            "Epoch 99/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.4739\n",
            "Epoch 100/100\n",
            "5886/5886 [==============================] - 1s 117us/step - loss: 1.4621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffb01d9ce10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVrtj7WrjIgQ",
        "colab_type": "code",
        "outputId": "ee7afb6f-a0b5-47ce-d22f-b90b0b1981be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,y_train,epochs=100,batch_size=300,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.4519\n",
            "Epoch 2/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.4480\n",
            "Epoch 3/100\n",
            "5886/5886 [==============================] - 1s 89us/step - loss: 1.4352\n",
            "Epoch 4/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.4405\n",
            "Epoch 5/100\n",
            "5886/5886 [==============================] - 0s 77us/step - loss: 1.4486\n",
            "Epoch 6/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.4318\n",
            "Epoch 7/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.4405\n",
            "Epoch 8/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.4317\n",
            "Epoch 9/100\n",
            "5886/5886 [==============================] - 1s 89us/step - loss: 1.4269\n",
            "Epoch 10/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.4303\n",
            "Epoch 11/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.4131\n",
            "Epoch 12/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.4207\n",
            "Epoch 13/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.4193\n",
            "Epoch 14/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.4193\n",
            "Epoch 15/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.4109\n",
            "Epoch 16/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.4174\n",
            "Epoch 17/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.4128\n",
            "Epoch 18/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.4138\n",
            "Epoch 19/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 1.4038\n",
            "Epoch 20/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.3929\n",
            "Epoch 21/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.4030\n",
            "Epoch 22/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.3876\n",
            "Epoch 23/100\n",
            "5886/5886 [==============================] - 1s 99us/step - loss: 1.3900\n",
            "Epoch 24/100\n",
            "5886/5886 [==============================] - 1s 94us/step - loss: 1.3900\n",
            "Epoch 25/100\n",
            "5886/5886 [==============================] - 1s 107us/step - loss: 1.3818\n",
            "Epoch 26/100\n",
            "5886/5886 [==============================] - 1s 108us/step - loss: 1.3761\n",
            "Epoch 27/100\n",
            "5886/5886 [==============================] - 1s 118us/step - loss: 1.3703\n",
            "Epoch 28/100\n",
            "5886/5886 [==============================] - 1s 96us/step - loss: 1.3825\n",
            "Epoch 29/100\n",
            "5886/5886 [==============================] - 1s 90us/step - loss: 1.3831\n",
            "Epoch 30/100\n",
            "5886/5886 [==============================] - 1s 92us/step - loss: 1.3726\n",
            "Epoch 31/100\n",
            "5886/5886 [==============================] - 1s 110us/step - loss: 1.3715\n",
            "Epoch 32/100\n",
            "5886/5886 [==============================] - 1s 119us/step - loss: 1.3670\n",
            "Epoch 33/100\n",
            "5886/5886 [==============================] - 1s 98us/step - loss: 1.3748\n",
            "Epoch 34/100\n",
            "5886/5886 [==============================] - 1s 90us/step - loss: 1.3631\n",
            "Epoch 35/100\n",
            "5886/5886 [==============================] - 1s 101us/step - loss: 1.3502\n",
            "Epoch 36/100\n",
            "5886/5886 [==============================] - 1s 120us/step - loss: 1.3632\n",
            "Epoch 37/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.3610\n",
            "Epoch 38/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.3551\n",
            "Epoch 39/100\n",
            "5886/5886 [==============================] - 1s 110us/step - loss: 1.3469\n",
            "Epoch 40/100\n",
            "5886/5886 [==============================] - 1s 126us/step - loss: 1.3435\n",
            "Epoch 41/100\n",
            "5886/5886 [==============================] - 1s 112us/step - loss: 1.3435\n",
            "Epoch 42/100\n",
            "5886/5886 [==============================] - 1s 122us/step - loss: 1.3436\n",
            "Epoch 43/100\n",
            "5886/5886 [==============================] - 1s 114us/step - loss: 1.3417\n",
            "Epoch 44/100\n",
            "5886/5886 [==============================] - 1s 91us/step - loss: 1.3407\n",
            "Epoch 45/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.3364\n",
            "Epoch 46/100\n",
            "5886/5886 [==============================] - 1s 87us/step - loss: 1.3453\n",
            "Epoch 47/100\n",
            "5886/5886 [==============================] - 1s 89us/step - loss: 1.3392\n",
            "Epoch 48/100\n",
            "5886/5886 [==============================] - 1s 121us/step - loss: 1.3181\n",
            "Epoch 49/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.3214\n",
            "Epoch 50/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.3285\n",
            "Epoch 51/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 1.3184\n",
            "Epoch 52/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.3302\n",
            "Epoch 53/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.3223\n",
            "Epoch 54/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.3072\n",
            "Epoch 55/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.3148\n",
            "Epoch 56/100\n",
            "5886/5886 [==============================] - 1s 87us/step - loss: 1.3203\n",
            "Epoch 57/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.3187\n",
            "Epoch 58/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.3071\n",
            "Epoch 59/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.3164\n",
            "Epoch 60/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.3035\n",
            "Epoch 61/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.3082\n",
            "Epoch 62/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2967\n",
            "Epoch 63/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.3126\n",
            "Epoch 64/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2997\n",
            "Epoch 65/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2936\n",
            "Epoch 66/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.3030\n",
            "Epoch 67/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.2857\n",
            "Epoch 68/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2910\n",
            "Epoch 69/100\n",
            "5886/5886 [==============================] - 1s 91us/step - loss: 1.2890\n",
            "Epoch 70/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2901\n",
            "Epoch 71/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.2818\n",
            "Epoch 72/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.2797\n",
            "Epoch 73/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2874\n",
            "Epoch 74/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.2828\n",
            "Epoch 75/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2801\n",
            "Epoch 76/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.2750\n",
            "Epoch 77/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2704\n",
            "Epoch 78/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2728\n",
            "Epoch 79/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2663\n",
            "Epoch 80/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.2687\n",
            "Epoch 81/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.2586\n",
            "Epoch 82/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2692\n",
            "Epoch 83/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.2724\n",
            "Epoch 84/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 1.2614\n",
            "Epoch 85/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.2563\n",
            "Epoch 86/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2524\n",
            "Epoch 87/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.2530\n",
            "Epoch 88/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.2501\n",
            "Epoch 89/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.2387\n",
            "Epoch 90/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.2462\n",
            "Epoch 91/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.2503\n",
            "Epoch 92/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.2363\n",
            "Epoch 93/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.2328\n",
            "Epoch 94/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.2320\n",
            "Epoch 95/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2509\n",
            "Epoch 96/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2362\n",
            "Epoch 97/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.2426\n",
            "Epoch 98/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.2409\n",
            "Epoch 99/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.2360\n",
            "Epoch 100/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.2300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffb01d9cda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt_syg4W0qv5",
        "colab_type": "code",
        "outputId": "289cf363-a9c6-4776-911a-91f3d862a084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,y_train,epochs=100,batch_size=300,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5886/5886 [==============================] - 0s 77us/step - loss: 1.2332\n",
            "Epoch 2/100\n",
            "5886/5886 [==============================] - 0s 77us/step - loss: 1.2354\n",
            "Epoch 3/100\n",
            "5886/5886 [==============================] - 0s 77us/step - loss: 1.2082\n",
            "Epoch 4/100\n",
            "5886/5886 [==============================] - 0s 78us/step - loss: 1.2322\n",
            "Epoch 5/100\n",
            "5886/5886 [==============================] - 0s 78us/step - loss: 1.2196\n",
            "Epoch 6/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2189\n",
            "Epoch 7/100\n",
            "5886/5886 [==============================] - 1s 87us/step - loss: 1.2198\n",
            "Epoch 8/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.2333\n",
            "Epoch 9/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.2125\n",
            "Epoch 10/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.2099\n",
            "Epoch 11/100\n",
            "5886/5886 [==============================] - 1s 87us/step - loss: 1.2147\n",
            "Epoch 12/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.2044\n",
            "Epoch 13/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.2112\n",
            "Epoch 14/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.2128\n",
            "Epoch 15/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.2158\n",
            "Epoch 16/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1936\n",
            "Epoch 17/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 1.2042\n",
            "Epoch 18/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1948\n",
            "Epoch 19/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.2102\n",
            "Epoch 20/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1929\n",
            "Epoch 21/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1964\n",
            "Epoch 22/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.1895\n",
            "Epoch 23/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1882\n",
            "Epoch 24/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.1791\n",
            "Epoch 25/100\n",
            "5886/5886 [==============================] - 1s 87us/step - loss: 1.1887\n",
            "Epoch 26/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1898\n",
            "Epoch 27/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1717\n",
            "Epoch 28/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1769\n",
            "Epoch 29/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1733\n",
            "Epoch 30/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1843\n",
            "Epoch 31/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.1803\n",
            "Epoch 32/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1690\n",
            "Epoch 33/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1719\n",
            "Epoch 34/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1791\n",
            "Epoch 35/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1601\n",
            "Epoch 36/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.1678\n",
            "Epoch 37/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1657\n",
            "Epoch 38/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1597\n",
            "Epoch 39/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.1656\n",
            "Epoch 40/100\n",
            "5886/5886 [==============================] - 1s 89us/step - loss: 1.1500\n",
            "Epoch 41/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.1594\n",
            "Epoch 42/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1667\n",
            "Epoch 43/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1602\n",
            "Epoch 44/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1461\n",
            "Epoch 45/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.1485\n",
            "Epoch 46/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1404\n",
            "Epoch 47/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1372\n",
            "Epoch 48/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1544\n",
            "Epoch 49/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.1577\n",
            "Epoch 50/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1463\n",
            "Epoch 51/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1413\n",
            "Epoch 52/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1468\n",
            "Epoch 53/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1509\n",
            "Epoch 54/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1266\n",
            "Epoch 55/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1220\n",
            "Epoch 56/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1274\n",
            "Epoch 57/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.1234\n",
            "Epoch 58/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.1287\n",
            "Epoch 59/100\n",
            "5886/5886 [==============================] - 0s 78us/step - loss: 1.1252\n",
            "Epoch 60/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1224\n",
            "Epoch 61/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1121\n",
            "Epoch 62/100\n",
            "5886/5886 [==============================] - 1s 87us/step - loss: 1.1257\n",
            "Epoch 63/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1080\n",
            "Epoch 64/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1216\n",
            "Epoch 65/100\n",
            "5886/5886 [==============================] - 0s 78us/step - loss: 1.1101\n",
            "Epoch 66/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1277\n",
            "Epoch 67/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1153\n",
            "Epoch 68/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.1039\n",
            "Epoch 69/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1167\n",
            "Epoch 70/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.1029\n",
            "Epoch 71/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1035\n",
            "Epoch 72/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.1068\n",
            "Epoch 73/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.1050\n",
            "Epoch 74/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.1124\n",
            "Epoch 75/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0835\n",
            "Epoch 76/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 1.1079\n",
            "Epoch 77/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0861\n",
            "Epoch 78/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0983\n",
            "Epoch 79/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.1067\n",
            "Epoch 80/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0891\n",
            "Epoch 81/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0898\n",
            "Epoch 82/100\n",
            "5886/5886 [==============================] - 0s 78us/step - loss: 1.0868\n",
            "Epoch 83/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0880\n",
            "Epoch 84/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0925\n",
            "Epoch 85/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 1.0898\n",
            "Epoch 86/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0765\n",
            "Epoch 87/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0845\n",
            "Epoch 88/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0719\n",
            "Epoch 89/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0708\n",
            "Epoch 90/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.0774\n",
            "Epoch 91/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0711\n",
            "Epoch 92/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0596\n",
            "Epoch 93/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0866\n",
            "Epoch 94/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.0849\n",
            "Epoch 95/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.0695\n",
            "Epoch 96/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0600\n",
            "Epoch 97/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0678\n",
            "Epoch 98/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0695\n",
            "Epoch 99/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0763\n",
            "Epoch 100/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.0656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffb01e110f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W28QFI4NCijO",
        "colab_type": "code",
        "outputId": "8c75d88a-e5e7-48df-e36d-58818bd57250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate the model\n",
        "model.evaluate(X_train, y_train, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5886/5886 [==============================] - 1s 152us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7860306589084136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0XpDUeUDlFd",
        "colab_type": "code",
        "outputId": "a6e3b792-ce2a-41af-9b42-011431f2ac44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,y_train,epochs=100,batch_size=300,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0543\n",
            "Epoch 2/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0648\n",
            "Epoch 3/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0635\n",
            "Epoch 4/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 1.0480\n",
            "Epoch 5/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 1.0639\n",
            "Epoch 6/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0495\n",
            "Epoch 7/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0709\n",
            "Epoch 8/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0562\n",
            "Epoch 9/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0645\n",
            "Epoch 10/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0529\n",
            "Epoch 11/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0413\n",
            "Epoch 12/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0388\n",
            "Epoch 13/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0415\n",
            "Epoch 14/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.0380\n",
            "Epoch 15/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0400\n",
            "Epoch 16/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 1.0384\n",
            "Epoch 17/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0209\n",
            "Epoch 18/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.0286\n",
            "Epoch 19/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0332\n",
            "Epoch 20/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0335\n",
            "Epoch 21/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0126\n",
            "Epoch 22/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.0372\n",
            "Epoch 23/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 1.0230\n",
            "Epoch 24/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0235\n",
            "Epoch 25/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0207\n",
            "Epoch 26/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0329\n",
            "Epoch 27/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0226\n",
            "Epoch 28/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0158\n",
            "Epoch 29/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0231\n",
            "Epoch 30/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0125\n",
            "Epoch 31/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 1.0031\n",
            "Epoch 32/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0018\n",
            "Epoch 33/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0098\n",
            "Epoch 34/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0104\n",
            "Epoch 35/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0039\n",
            "Epoch 36/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0124\n",
            "Epoch 37/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0026\n",
            "Epoch 38/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9967\n",
            "Epoch 39/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 1.0138\n",
            "Epoch 40/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 1.0011\n",
            "Epoch 41/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0037\n",
            "Epoch 42/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 1.0004\n",
            "Epoch 43/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 1.0030\n",
            "Epoch 44/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9869\n",
            "Epoch 45/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 0.9960\n",
            "Epoch 46/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 0.9894\n",
            "Epoch 47/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 0.9835\n",
            "Epoch 48/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9930\n",
            "Epoch 49/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9831\n",
            "Epoch 50/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9909\n",
            "Epoch 51/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9850\n",
            "Epoch 52/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 0.9894\n",
            "Epoch 53/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9849\n",
            "Epoch 54/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9831\n",
            "Epoch 55/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 0.9816\n",
            "Epoch 56/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9759\n",
            "Epoch 57/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9691\n",
            "Epoch 58/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 0.9788\n",
            "Epoch 59/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9727\n",
            "Epoch 60/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9733\n",
            "Epoch 61/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9749\n",
            "Epoch 62/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 0.9641\n",
            "Epoch 63/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9836\n",
            "Epoch 64/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9680\n",
            "Epoch 65/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 0.9500\n",
            "Epoch 66/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9833\n",
            "Epoch 67/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 0.9690\n",
            "Epoch 68/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9463\n",
            "Epoch 69/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9589\n",
            "Epoch 70/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9634\n",
            "Epoch 71/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 0.9624\n",
            "Epoch 72/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9684\n",
            "Epoch 73/100\n",
            "5886/5886 [==============================] - 1s 85us/step - loss: 0.9647\n",
            "Epoch 74/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 0.9484\n",
            "Epoch 75/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 0.9553\n",
            "Epoch 76/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 0.9517\n",
            "Epoch 77/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9449\n",
            "Epoch 78/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9680\n",
            "Epoch 79/100\n",
            "5886/5886 [==============================] - 1s 86us/step - loss: 0.9561\n",
            "Epoch 80/100\n",
            "5886/5886 [==============================] - 0s 85us/step - loss: 0.9514\n",
            "Epoch 81/100\n",
            "5886/5886 [==============================] - 0s 78us/step - loss: 0.9491\n",
            "Epoch 82/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9488\n",
            "Epoch 83/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9456\n",
            "Epoch 84/100\n",
            "5886/5886 [==============================] - 1s 88us/step - loss: 0.9463\n",
            "Epoch 85/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9491\n",
            "Epoch 86/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9480\n",
            "Epoch 87/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9326\n",
            "Epoch 88/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9400\n",
            "Epoch 89/100\n",
            "5886/5886 [==============================] - 1s 87us/step - loss: 0.9434\n",
            "Epoch 90/100\n",
            "5886/5886 [==============================] - 0s 80us/step - loss: 0.9291\n",
            "Epoch 91/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9449\n",
            "Epoch 92/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9422\n",
            "Epoch 93/100\n",
            "5886/5886 [==============================] - 0s 83us/step - loss: 0.9409\n",
            "Epoch 94/100\n",
            "5886/5886 [==============================] - 0s 79us/step - loss: 0.9417\n",
            "Epoch 95/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9249\n",
            "Epoch 96/100\n",
            "5886/5886 [==============================] - 0s 82us/step - loss: 0.9247\n",
            "Epoch 97/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9436\n",
            "Epoch 98/100\n",
            "5886/5886 [==============================] - 0s 81us/step - loss: 0.9168\n",
            "Epoch 99/100\n",
            "5886/5886 [==============================] - 0s 84us/step - loss: 0.9438\n",
            "Epoch 100/100\n",
            "5886/5886 [==============================] - 0s 78us/step - loss: 0.9314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffafe847400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2W0VyyHD8kN",
        "colab_type": "code",
        "outputId": "986aeba8-4761-4296-ed2c-47a493a20711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate the model\n",
        "model.evaluate(X_train, y_train, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5886/5886 [==============================] - 1s 105us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14993418873146283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ISXcL2rEQOZ",
        "colab_type": "code",
        "outputId": "b95e492f-74ac-4ad6-b93d-c5ee2437f981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,y_train,epochs=100,batch_size=400,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3553\n",
            "Epoch 2/100\n",
            "5886/5886 [==============================] - 0s 61us/step - loss: 0.3612\n",
            "Epoch 3/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3606\n",
            "Epoch 4/100\n",
            "5886/5886 [==============================] - 0s 59us/step - loss: 0.3552\n",
            "Epoch 5/100\n",
            "5886/5886 [==============================] - 0s 60us/step - loss: 0.3569\n",
            "Epoch 6/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3600\n",
            "Epoch 7/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3564\n",
            "Epoch 8/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3607\n",
            "Epoch 9/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3553\n",
            "Epoch 10/100\n",
            "5886/5886 [==============================] - 0s 70us/step - loss: 0.3569\n",
            "Epoch 11/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3591\n",
            "Epoch 12/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3509\n",
            "Epoch 13/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3584\n",
            "Epoch 14/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3502\n",
            "Epoch 15/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3460\n",
            "Epoch 16/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3505\n",
            "Epoch 17/100\n",
            "5886/5886 [==============================] - 0s 63us/step - loss: 0.3512\n",
            "Epoch 18/100\n",
            "5886/5886 [==============================] - 0s 70us/step - loss: 0.3523\n",
            "Epoch 19/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3515\n",
            "Epoch 20/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3651\n",
            "Epoch 21/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3626\n",
            "Epoch 22/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3574\n",
            "Epoch 23/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3491\n",
            "Epoch 24/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3634\n",
            "Epoch 25/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3471\n",
            "Epoch 26/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3639\n",
            "Epoch 27/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3568\n",
            "Epoch 28/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3526\n",
            "Epoch 29/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3527\n",
            "Epoch 30/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3545\n",
            "Epoch 31/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3568\n",
            "Epoch 32/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3632\n",
            "Epoch 33/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3601\n",
            "Epoch 34/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3533\n",
            "Epoch 35/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3587\n",
            "Epoch 36/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3558\n",
            "Epoch 37/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3485\n",
            "Epoch 38/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3568\n",
            "Epoch 39/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3526\n",
            "Epoch 40/100\n",
            "5886/5886 [==============================] - 0s 62us/step - loss: 0.3549\n",
            "Epoch 41/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3518\n",
            "Epoch 42/100\n",
            "5886/5886 [==============================] - 0s 63us/step - loss: 0.3592\n",
            "Epoch 43/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3566\n",
            "Epoch 44/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3520\n",
            "Epoch 45/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3532\n",
            "Epoch 46/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3584\n",
            "Epoch 47/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3442\n",
            "Epoch 48/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3539\n",
            "Epoch 49/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3608\n",
            "Epoch 50/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3449\n",
            "Epoch 51/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3521\n",
            "Epoch 52/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3608\n",
            "Epoch 53/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3452\n",
            "Epoch 54/100\n",
            "5886/5886 [==============================] - 0s 70us/step - loss: 0.3481\n",
            "Epoch 55/100\n",
            "5886/5886 [==============================] - 0s 63us/step - loss: 0.3521\n",
            "Epoch 56/100\n",
            "5886/5886 [==============================] - 0s 63us/step - loss: 0.3593\n",
            "Epoch 57/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3624\n",
            "Epoch 58/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3600\n",
            "Epoch 59/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3528\n",
            "Epoch 60/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3513\n",
            "Epoch 61/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3550\n",
            "Epoch 62/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3489\n",
            "Epoch 63/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3482\n",
            "Epoch 64/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3503\n",
            "Epoch 65/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3605\n",
            "Epoch 66/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3547\n",
            "Epoch 67/100\n",
            "5886/5886 [==============================] - 0s 63us/step - loss: 0.3512\n",
            "Epoch 68/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3497\n",
            "Epoch 69/100\n",
            "5886/5886 [==============================] - 0s 63us/step - loss: 0.3524\n",
            "Epoch 70/100\n",
            "5886/5886 [==============================] - 0s 71us/step - loss: 0.3525\n",
            "Epoch 71/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3485\n",
            "Epoch 72/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3486\n",
            "Epoch 73/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3496\n",
            "Epoch 74/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3473\n",
            "Epoch 75/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3447\n",
            "Epoch 76/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3498\n",
            "Epoch 77/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3530\n",
            "Epoch 78/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3478\n",
            "Epoch 79/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3526\n",
            "Epoch 80/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3538\n",
            "Epoch 81/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3480\n",
            "Epoch 82/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3580\n",
            "Epoch 83/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3529\n",
            "Epoch 84/100\n",
            "5886/5886 [==============================] - 0s 65us/step - loss: 0.3486\n",
            "Epoch 85/100\n",
            "5886/5886 [==============================] - 0s 61us/step - loss: 0.3518\n",
            "Epoch 86/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3529\n",
            "Epoch 87/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3501\n",
            "Epoch 88/100\n",
            "5886/5886 [==============================] - 0s 69us/step - loss: 0.3568\n",
            "Epoch 89/100\n",
            "5886/5886 [==============================] - 0s 64us/step - loss: 0.3399\n",
            "Epoch 90/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3532\n",
            "Epoch 91/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3427\n",
            "Epoch 92/100\n",
            "5886/5886 [==============================] - 0s 71us/step - loss: 0.3466\n",
            "Epoch 93/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3391\n",
            "Epoch 94/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3460\n",
            "Epoch 95/100\n",
            "5886/5886 [==============================] - 0s 68us/step - loss: 0.3457\n",
            "Epoch 96/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3617\n",
            "Epoch 97/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3543\n",
            "Epoch 98/100\n",
            "5886/5886 [==============================] - 0s 62us/step - loss: 0.3480\n",
            "Epoch 99/100\n",
            "5886/5886 [==============================] - 0s 67us/step - loss: 0.3521\n",
            "Epoch 100/100\n",
            "5886/5886 [==============================] - 0s 66us/step - loss: 0.3551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffafc0ff438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4BnLEzGCv3k",
        "colab_type": "code",
        "outputId": "7506b83e-6241-48cf-96be-cbfe8237f3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = './NN_models/3'\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(str(path)+'/model.json', \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "# serialize weights to HDF5\n",
        "model.save_weights(str(path)+'/model.h5')\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So3N8UQ7C0VL",
        "colab_type": "code",
        "outputId": "42ac184f-7d91-4cb7-f3c5-6a53d0c72dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('./NN_models/3/model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"./NN_models/3/model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# # evaluate loaded model on test data\n",
        "# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0816 16:26:53.884556 139837236975488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0816 16:26:53.901741 139837236975488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0816 16:26:53.994660 139837236975488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0816 16:26:53.996046 139837236975488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0816 16:26:54.139410 139837236975488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0816 16:26:55.174657 139837236975488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRRl3lHGKZxc",
        "colab_type": "code",
        "outputId": "14181503-88fc-42bb-ebcb-2bcb261029b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "predicts = model.predict(X_test)\n",
        "\n",
        "Y = []\n",
        "pr= []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    \n",
        "    pr.append(idxmax(predicts[i]))\n",
        "    \n",
        "    Y.append(np.argmax(y_test[i]))\n",
        "\n",
        "\n",
        "labels = [class_map[i][0] for i in range(len(class_map))]  \n",
        "    \n",
        "conf_plotter(Y , pr , labels , normalize=1, mode='test' , cmap = plt.cm.YlGn, save=0 , name='test_3.jpg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-573e0233d112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lw5xuyqTsuX",
        "colab_type": "code",
        "outputId": "bfaeb02b-5b2b-4381-f1a7-0fa6fa160571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(Y, pr)\n",
        "np.trace(cm)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0ee351e5d5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5IdCppVRRv8",
        "colab_type": "code",
        "outputId": "47297184-c21b-4f8c-d3fb-ee3190deafce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(predicts.shape)\n",
        "print(np.array(Y).shape)\n",
        "print(np.array(pr).shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1962, 14)\n",
            "(1962,)\n",
            "(1962,)\n",
            "(1962, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JDfB7hGx1le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = pd.DataFrame(y_test)\n",
        "y_preds = pd.DataFrame(predicts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1XM4XwQb3j",
        "colab_type": "code",
        "outputId": "cb6a169d-bb6a-41ec-960b-ac41a87b5c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def multi_weighted_logloss(y_true, y_preds):\n",
        "    \"\"\"\n",
        "    @author olivier https://www.kaggle.com/ogrellier\n",
        "    multi logloss for PLAsTiCC challenge\n",
        "    \"\"\"\n",
        "    classes = targets.unique()\n",
        "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
        "    \n",
        "    \n",
        "    y_p = y_preds\n",
        "    \n",
        "    y_ohe = pd.get_dummies(y_true)\n",
        "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
        "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
        "    # Transform to log\n",
        "    y_p_log = np.log(y_p)\n",
        "\n",
        "    \n",
        "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
        "    # Get the number of positives for each class\n",
        "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
        "    # Weight average and divide by the number of positives\n",
        "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
        "    y_w = y_log_ones * class_arr / nb_pos\n",
        "    \n",
        "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
        "    return loss\n",
        "\n",
        "# s = 0 \n",
        "# for i in range(len(y_test)):\n",
        "#     s += multi_weighted_logloss( Y[i] , pr[i] )\n",
        "# s/len(y_test)\n",
        "\n",
        "multi_weighted_logloss(y_true, y_preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2603242689698757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELtxr4col72p",
        "colab_type": "code",
        "outputId": "cc913e49-26f8-4454-ea94-94278848a64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        }
      },
      "source": [
        "predicts = model.predict(X_train)\n",
        "\n",
        "Y = []\n",
        "pr= []\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    \n",
        "    pr.append(np.argmax(predicts[i]))\n",
        "    Y.append(np.argmax(y_train[i]))\n",
        "\n",
        "\n",
        "labels = [class_map[i][0] for i in range(len(class_map))]   \n",
        "    \n",
        "conf_plotter(Y , pr , labels , normalize=1, cmap = plt.cm.OrRd, save=1 , name='train_3.jpg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ffafbb0b7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAKVCAYAAADSux1fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZmZWcX/A5jIq/Myhw+8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccnnbt-7qBXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHdmOYc4ZBpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}