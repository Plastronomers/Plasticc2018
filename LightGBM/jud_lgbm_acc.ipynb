{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jud_lgbm_acc",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoJcX2bK4rGw",
        "colab_type": "code",
        "outputId": "1251af23-6304-4c06-c973-be2c42899300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp7ZJpTY4-uU",
        "colab_type": "code",
        "outputId": "a126ce19-08d5-4d09-a57c-28b1e64ae1d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "pip install scikit-optimize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.13.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V56ju9J5CIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline \n",
        "\n",
        "import pylab as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import collections\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import gc\n",
        "import os\n",
        "import itertools\n",
        " \n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt import gp_minimize, forest_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.plots import plot_convergence\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nco5mtUY5R-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_weighted_logloss(y_true, y_preds):\n",
        "    \"\"\"\n",
        "    @author olivier https://www.kaggle.com/ogrellier\n",
        "    multi logloss for PLAsTiCC challenge\n",
        "    \"\"\"\n",
        "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
        "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
        "    if len(np.unique(y_true)) > 14:\n",
        "        classes.append(99)\n",
        "        class_weight[99] = 2\n",
        "    y_p = y_preds\n",
        "    \n",
        "    y_ohe = pd.get_dummies(y_true)\n",
        "    \n",
        "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
        "    \n",
        "    y_p_log = np.log(y_p)\n",
        "\n",
        "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
        "\n",
        "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
        "    \n",
        "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
        "    y_w = y_log_ones * class_arr / nb_pos\n",
        "    \n",
        "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISV2n-Gx5Uqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
        "    \"\"\"\n",
        "    @author olivier https://www.kaggle.com/ogrellier\n",
        "    multi logloss for PLAsTiCC challenge\n",
        "    \"\"\"\n",
        "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
        "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
        "    if len(np.unique(y_true)) > 14:\n",
        "        classes.append(99)\n",
        "        class_weight[99] = 2\n",
        "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
        "    \n",
        "    # Trasform y_true in dummies\n",
        "    y_ohe = pd.get_dummies(y_true)\n",
        "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
        "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
        "    # Transform to log\n",
        "    y_p_log = np.log(y_p)\n",
        "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
        "    # Exclude class 99 for now, since there is no class99 in the training set \n",
        "    # we gave a special process for that class\n",
        "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
        "    # Get the number of positives for each class\n",
        "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
        "    # Weight average and divide by the number of positives\n",
        "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
        "    y_w = y_log_ones * class_arr / nb_pos\n",
        "    \n",
        "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
        "    return 'wloss', loss, False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQa6jiCjEnnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pd.read_csv('/data/X_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBxQcXd6En70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = pd.read_csv('/data/X_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQI3BaSLEod8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = pd.read_csv('/data/y_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ4UUhadGoJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = pd.read_csv('/data/y_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6JqjEut5f5V",
        "colab_type": "code",
        "outputId": "f347386f-298a-4209-de55-a2bc75988e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "del y_train['Unnamed: 0']\n",
        "y_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target\n",
              "0      95\n",
              "1      15\n",
              "2      42\n",
              "3      65\n",
              "4      90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipjerscyHBpU",
        "colab_type": "code",
        "outputId": "5bf538da-aa60-4f6e-b190-a9f444cafde1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "del y_test['Unnamed: 0']\n",
        "y_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target\n",
              "0      90\n",
              "1      16\n",
              "2      90\n",
              "3      90\n",
              "4      52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50_9x3RiHB-p",
        "colab_type": "code",
        "outputId": "c615f075-c8be-4154-d1a5-2f477d24ecfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "del x_train['Unnamed: 0']\n",
        "x_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_ratio_sq_sum</th>\n",
              "      <th>flux_ratio_sq_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>flux_w_mean</th>\n",
              "      <th>flux_diff1</th>\n",
              "      <th>flux_diff2</th>\n",
              "      <th>flux_diff3</th>\n",
              "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>0__kurtosis</th>\n",
              "      <th>0__skewness</th>\n",
              "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>1__kurtosis</th>\n",
              "      <th>1__skewness</th>\n",
              "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>2__kurtosis</th>\n",
              "      <th>2__skewness</th>\n",
              "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>3__kurtosis</th>\n",
              "      <th>3__skewness</th>\n",
              "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>4__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>4__kurtosis</th>\n",
              "      <th>...</th>\n",
              "      <th>__freq_signif_ratio_21___2_</th>\n",
              "      <th>__freq_signif_ratio_21___3_</th>\n",
              "      <th>__freq_signif_ratio_21___4_</th>\n",
              "      <th>__freq_signif_ratio_21___5_</th>\n",
              "      <th>__freq_signif_ratio_31___0_</th>\n",
              "      <th>__freq_signif_ratio_31___1_</th>\n",
              "      <th>__freq_signif_ratio_31___2_</th>\n",
              "      <th>__freq_signif_ratio_31___3_</th>\n",
              "      <th>__freq_signif_ratio_31___4_</th>\n",
              "      <th>__freq_signif_ratio_31___5_</th>\n",
              "      <th>__freq_varrat___0_</th>\n",
              "      <th>__freq_varrat___1_</th>\n",
              "      <th>__freq_varrat___2_</th>\n",
              "      <th>__freq_varrat___3_</th>\n",
              "      <th>__freq_varrat___4_</th>\n",
              "      <th>__freq_varrat___5_</th>\n",
              "      <th>__freq_y_offset___0_</th>\n",
              "      <th>__freq_y_offset___1_</th>\n",
              "      <th>__freq_y_offset___2_</th>\n",
              "      <th>__freq_y_offset___3_</th>\n",
              "      <th>__freq_y_offset___4_</th>\n",
              "      <th>__freq_y_offset___5_</th>\n",
              "      <th>time_score</th>\n",
              "      <th>phase_score</th>\n",
              "      <th>ddf_bool</th>\n",
              "      <th>true_submodel</th>\n",
              "      <th>true_z</th>\n",
              "      <th>true_distmod</th>\n",
              "      <th>true_lensdmu</th>\n",
              "      <th>true_vpec</th>\n",
              "      <th>true_rv</th>\n",
              "      <th>true_av</th>\n",
              "      <th>true_peakmjd</th>\n",
              "      <th>libid_cadence</th>\n",
              "      <th>tflux_u</th>\n",
              "      <th>tflux_g</th>\n",
              "      <th>tflux_r</th>\n",
              "      <th>tflux_i</th>\n",
              "      <th>tflux_z</th>\n",
              "      <th>tflux_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.999518</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.254382</td>\n",
              "      <td>0.584163</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.500566</td>\n",
              "      <td>0.002111</td>\n",
              "      <td>1.953305e-05</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.001765</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.219335</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.002637</td>\n",
              "      <td>0.607150</td>\n",
              "      <td>0.423427</td>\n",
              "      <td>0.794889</td>\n",
              "      <td>0.281841</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.522498</td>\n",
              "      <td>0.540702</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.072829</td>\n",
              "      <td>0.544026</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.238292</td>\n",
              "      <td>0.705124</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.353401</td>\n",
              "      <td>0.777249</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.069519</td>\n",
              "      <td>0.604292</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.095569</td>\n",
              "      <td>...</td>\n",
              "      <td>0.423011</td>\n",
              "      <td>0.713125</td>\n",
              "      <td>0.671420</td>\n",
              "      <td>0.659257</td>\n",
              "      <td>0.526549</td>\n",
              "      <td>0.446278</td>\n",
              "      <td>0.539195</td>\n",
              "      <td>0.687463</td>\n",
              "      <td>0.709204</td>\n",
              "      <td>0.664974</td>\n",
              "      <td>0.876364</td>\n",
              "      <td>0.599403</td>\n",
              "      <td>0.449503</td>\n",
              "      <td>0.081471</td>\n",
              "      <td>0.248672</td>\n",
              "      <td>0.243160</td>\n",
              "      <td>0.224296</td>\n",
              "      <td>0.485048</td>\n",
              "      <td>0.999926</td>\n",
              "      <td>0.466091</td>\n",
              "      <td>0.564670</td>\n",
              "      <td>0.438104</td>\n",
              "      <td>0.803850</td>\n",
              "      <td>0.300852</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341272</td>\n",
              "      <td>0.939813</td>\n",
              "      <td>0.832041</td>\n",
              "      <td>0.496225</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.064807</td>\n",
              "      <td>0.475067</td>\n",
              "      <td>0.267735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.999522</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.254260</td>\n",
              "      <td>0.584133</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.474777</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>1.708378e-05</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.002183</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.208700</td>\n",
              "      <td>0.125802</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.340620</td>\n",
              "      <td>0.423396</td>\n",
              "      <td>0.655382</td>\n",
              "      <td>0.281020</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.522494</td>\n",
              "      <td>0.540735</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.040904</td>\n",
              "      <td>0.552777</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.197187</td>\n",
              "      <td>0.681010</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.067372</td>\n",
              "      <td>0.619056</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.105535</td>\n",
              "      <td>0.650877</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.073348</td>\n",
              "      <td>...</td>\n",
              "      <td>0.482407</td>\n",
              "      <td>0.720479</td>\n",
              "      <td>0.694003</td>\n",
              "      <td>0.651872</td>\n",
              "      <td>0.560532</td>\n",
              "      <td>0.541420</td>\n",
              "      <td>0.646418</td>\n",
              "      <td>0.722628</td>\n",
              "      <td>0.737486</td>\n",
              "      <td>0.660249</td>\n",
              "      <td>0.697940</td>\n",
              "      <td>0.523714</td>\n",
              "      <td>0.444721</td>\n",
              "      <td>0.454799</td>\n",
              "      <td>0.812657</td>\n",
              "      <td>0.700912</td>\n",
              "      <td>0.261547</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.999926</td>\n",
              "      <td>0.472914</td>\n",
              "      <td>0.557677</td>\n",
              "      <td>0.438204</td>\n",
              "      <td>0.700257</td>\n",
              "      <td>0.339619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.118792</td>\n",
              "      <td>0.880956</td>\n",
              "      <td>0.731266</td>\n",
              "      <td>0.364159</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.175966</td>\n",
              "      <td>0.665921</td>\n",
              "      <td>0.851698</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.999540</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.254140</td>\n",
              "      <td>0.584093</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.471625</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>1.004257e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.242679</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.286056</td>\n",
              "      <td>0.423389</td>\n",
              "      <td>0.632546</td>\n",
              "      <td>0.280440</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.522619</td>\n",
              "      <td>0.541205</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.062065</td>\n",
              "      <td>0.492872</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.106828</td>\n",
              "      <td>0.538449</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.041643</td>\n",
              "      <td>0.466159</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.033918</td>\n",
              "      <td>0.520340</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.026261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.464337</td>\n",
              "      <td>0.701253</td>\n",
              "      <td>0.443112</td>\n",
              "      <td>0.607159</td>\n",
              "      <td>0.484540</td>\n",
              "      <td>0.448724</td>\n",
              "      <td>0.438772</td>\n",
              "      <td>0.567787</td>\n",
              "      <td>0.473089</td>\n",
              "      <td>0.630286</td>\n",
              "      <td>0.662972</td>\n",
              "      <td>0.678980</td>\n",
              "      <td>0.572090</td>\n",
              "      <td>0.454655</td>\n",
              "      <td>0.144208</td>\n",
              "      <td>0.377865</td>\n",
              "      <td>0.224303</td>\n",
              "      <td>0.476947</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.462460</td>\n",
              "      <td>0.550681</td>\n",
              "      <td>0.437032</td>\n",
              "      <td>0.455794</td>\n",
              "      <td>0.261640</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126343</td>\n",
              "      <td>0.884270</td>\n",
              "      <td>0.811370</td>\n",
              "      <td>0.411034</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.699887</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.999516</td>\n",
              "      <td>0.000364</td>\n",
              "      <td>0.254150</td>\n",
              "      <td>0.584095</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.484985</td>\n",
              "      <td>0.003920</td>\n",
              "      <td>1.760178e-05</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.182780</td>\n",
              "      <td>0.009185</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.544657</td>\n",
              "      <td>0.423389</td>\n",
              "      <td>0.806399</td>\n",
              "      <td>0.280657</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.522796</td>\n",
              "      <td>0.540914</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.048249</td>\n",
              "      <td>0.453378</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>0.482592</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.061257</td>\n",
              "      <td>0.563261</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.018081</td>\n",
              "      <td>0.507944</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.305252</td>\n",
              "      <td>...</td>\n",
              "      <td>0.519952</td>\n",
              "      <td>0.725559</td>\n",
              "      <td>0.621668</td>\n",
              "      <td>0.657569</td>\n",
              "      <td>0.592650</td>\n",
              "      <td>0.445010</td>\n",
              "      <td>0.668307</td>\n",
              "      <td>0.721880</td>\n",
              "      <td>0.646322</td>\n",
              "      <td>0.663628</td>\n",
              "      <td>0.837491</td>\n",
              "      <td>0.325858</td>\n",
              "      <td>0.195972</td>\n",
              "      <td>0.682186</td>\n",
              "      <td>0.860254</td>\n",
              "      <td>0.807625</td>\n",
              "      <td>0.219890</td>\n",
              "      <td>0.477281</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.461495</td>\n",
              "      <td>0.554596</td>\n",
              "      <td>0.437203</td>\n",
              "      <td>0.292762</td>\n",
              "      <td>0.266709</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.793282</td>\n",
              "      <td>0.481390</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.453353</td>\n",
              "      <td>0.654703</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>0.001603</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.011018</td>\n",
              "      <td>0.012651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.999544</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.254137</td>\n",
              "      <td>0.584093</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.477098</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>9.646799e-07</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.244594</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.324852</td>\n",
              "      <td>0.423389</td>\n",
              "      <td>0.741880</td>\n",
              "      <td>0.280426</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.522702</td>\n",
              "      <td>0.541496</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.048111</td>\n",
              "      <td>0.514181</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.081140</td>\n",
              "      <td>0.506750</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.050086</td>\n",
              "      <td>0.542176</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.111138</td>\n",
              "      <td>0.569721</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.083792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.522701</td>\n",
              "      <td>0.656962</td>\n",
              "      <td>0.757961</td>\n",
              "      <td>0.697686</td>\n",
              "      <td>0.488083</td>\n",
              "      <td>0.412280</td>\n",
              "      <td>0.562490</td>\n",
              "      <td>0.707553</td>\n",
              "      <td>0.640107</td>\n",
              "      <td>0.691658</td>\n",
              "      <td>0.702864</td>\n",
              "      <td>0.581293</td>\n",
              "      <td>0.422081</td>\n",
              "      <td>0.383480</td>\n",
              "      <td>0.459741</td>\n",
              "      <td>0.556834</td>\n",
              "      <td>0.222931</td>\n",
              "      <td>0.476836</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.462562</td>\n",
              "      <td>0.552520</td>\n",
              "      <td>0.438996</td>\n",
              "      <td>0.515543</td>\n",
              "      <td>0.369605</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.210863</td>\n",
              "      <td>0.912675</td>\n",
              "      <td>0.850129</td>\n",
              "      <td>0.632725</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.517319</td>\n",
              "      <td>0.001340</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 368 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   flux_min  flux_max  flux_mean  ...   tflux_i   tflux_z   tflux_y\n",
              "0  0.999518  0.000513   0.254382  ...  0.000000  0.000000  0.000000\n",
              "1  0.999522  0.000401   0.254260  ...  0.000000  0.000000  0.000000\n",
              "2  0.999540  0.000321   0.254140  ...  0.000000  0.000000  0.000000\n",
              "3  0.999516  0.000364   0.254150  ...  0.007041  0.011018  0.012651\n",
              "4  0.999544  0.000322   0.254137  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[5 rows x 368 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioOo5DBgHCVT",
        "colab_type": "code",
        "outputId": "5ebcc00a-23f0-448e-8bdd-070327ea7e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "del x_test['Unnamed: 0']\n",
        "x_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_ratio_sq_sum</th>\n",
              "      <th>flux_ratio_sq_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>flux_w_mean</th>\n",
              "      <th>flux_diff1</th>\n",
              "      <th>flux_diff2</th>\n",
              "      <th>flux_diff3</th>\n",
              "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>0__kurtosis</th>\n",
              "      <th>0__skewness</th>\n",
              "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>1__kurtosis</th>\n",
              "      <th>1__skewness</th>\n",
              "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>2__kurtosis</th>\n",
              "      <th>2__skewness</th>\n",
              "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>3__kurtosis</th>\n",
              "      <th>3__skewness</th>\n",
              "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
              "      <th>4__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
              "      <th>4__kurtosis</th>\n",
              "      <th>...</th>\n",
              "      <th>__freq_signif_ratio_21___2_</th>\n",
              "      <th>__freq_signif_ratio_21___3_</th>\n",
              "      <th>__freq_signif_ratio_21___4_</th>\n",
              "      <th>__freq_signif_ratio_21___5_</th>\n",
              "      <th>__freq_signif_ratio_31___0_</th>\n",
              "      <th>__freq_signif_ratio_31___1_</th>\n",
              "      <th>__freq_signif_ratio_31___2_</th>\n",
              "      <th>__freq_signif_ratio_31___3_</th>\n",
              "      <th>__freq_signif_ratio_31___4_</th>\n",
              "      <th>__freq_signif_ratio_31___5_</th>\n",
              "      <th>__freq_varrat___0_</th>\n",
              "      <th>__freq_varrat___1_</th>\n",
              "      <th>__freq_varrat___2_</th>\n",
              "      <th>__freq_varrat___3_</th>\n",
              "      <th>__freq_varrat___4_</th>\n",
              "      <th>__freq_varrat___5_</th>\n",
              "      <th>__freq_y_offset___0_</th>\n",
              "      <th>__freq_y_offset___1_</th>\n",
              "      <th>__freq_y_offset___2_</th>\n",
              "      <th>__freq_y_offset___3_</th>\n",
              "      <th>__freq_y_offset___4_</th>\n",
              "      <th>__freq_y_offset___5_</th>\n",
              "      <th>time_score</th>\n",
              "      <th>phase_score</th>\n",
              "      <th>ddf_bool</th>\n",
              "      <th>true_submodel</th>\n",
              "      <th>true_z</th>\n",
              "      <th>true_distmod</th>\n",
              "      <th>true_lensdmu</th>\n",
              "      <th>true_vpec</th>\n",
              "      <th>true_rv</th>\n",
              "      <th>true_av</th>\n",
              "      <th>true_peakmjd</th>\n",
              "      <th>libid_cadence</th>\n",
              "      <th>tflux_u</th>\n",
              "      <th>tflux_g</th>\n",
              "      <th>tflux_r</th>\n",
              "      <th>tflux_i</th>\n",
              "      <th>tflux_z</th>\n",
              "      <th>tflux_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.999515</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.254149</td>\n",
              "      <td>0.584127</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.435582</td>\n",
              "      <td>0.005721</td>\n",
              "      <td>1.742738e-05</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.001725</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.239027</td>\n",
              "      <td>0.012571</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.506183</td>\n",
              "      <td>0.423389</td>\n",
              "      <td>0.758133</td>\n",
              "      <td>0.280534</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.522696</td>\n",
              "      <td>0.541066</td>\n",
              "      <td>9.558874e-06</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.048412</td>\n",
              "      <td>0.429062</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.083525</td>\n",
              "      <td>0.479756</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.039061</td>\n",
              "      <td>0.442239</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.032180</td>\n",
              "      <td>0.492836</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.079768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.502601</td>\n",
              "      <td>0.717468</td>\n",
              "      <td>0.674961</td>\n",
              "      <td>0.684765</td>\n",
              "      <td>0.566793</td>\n",
              "      <td>0.468399</td>\n",
              "      <td>0.618046</td>\n",
              "      <td>0.722902</td>\n",
              "      <td>0.715721</td>\n",
              "      <td>0.690469</td>\n",
              "      <td>0.828365</td>\n",
              "      <td>0.778785</td>\n",
              "      <td>0.387286</td>\n",
              "      <td>0.547306</td>\n",
              "      <td>0.379549</td>\n",
              "      <td>0.894386</td>\n",
              "      <td>0.222339</td>\n",
              "      <td>0.477114</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.461782</td>\n",
              "      <td>0.554292</td>\n",
              "      <td>0.437821</td>\n",
              "      <td>0.365501</td>\n",
              "      <td>0.231063</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126053</td>\n",
              "      <td>0.884016</td>\n",
              "      <td>0.793282</td>\n",
              "      <td>0.132417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.508101</td>\n",
              "      <td>0.452567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.994485</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.253550</td>\n",
              "      <td>0.584075</td>\n",
              "      <td>0.002307</td>\n",
              "      <td>0.183037</td>\n",
              "      <td>0.021144</td>\n",
              "      <td>2.205954e-05</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.004627</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.216118</td>\n",
              "      <td>0.035504</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>0.359390</td>\n",
              "      <td>0.422889</td>\n",
              "      <td>0.293875</td>\n",
              "      <td>0.266502</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.521985</td>\n",
              "      <td>0.540183</td>\n",
              "      <td>3.117481e-05</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.028351</td>\n",
              "      <td>0.494800</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.075344</td>\n",
              "      <td>0.437439</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.002444</td>\n",
              "      <td>0.468398</td>\n",
              "      <td>0.166394</td>\n",
              "      <td>0.001685</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.192322</td>\n",
              "      <td>0.282811</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>0.497944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.535448</td>\n",
              "      <td>0.695403</td>\n",
              "      <td>0.667335</td>\n",
              "      <td>0.654560</td>\n",
              "      <td>0.576056</td>\n",
              "      <td>0.516025</td>\n",
              "      <td>0.689422</td>\n",
              "      <td>0.697256</td>\n",
              "      <td>0.697105</td>\n",
              "      <td>0.658709</td>\n",
              "      <td>0.671461</td>\n",
              "      <td>0.891587</td>\n",
              "      <td>0.976274</td>\n",
              "      <td>0.960832</td>\n",
              "      <td>0.975856</td>\n",
              "      <td>0.954428</td>\n",
              "      <td>0.221715</td>\n",
              "      <td>0.477126</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.449158</td>\n",
              "      <td>0.548034</td>\n",
              "      <td>0.437850</td>\n",
              "      <td>0.143027</td>\n",
              "      <td>0.319818</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.793282</td>\n",
              "      <td>0.481390</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.453353</td>\n",
              "      <td>0.727304</td>\n",
              "      <td>0.155725</td>\n",
              "      <td>0.163516</td>\n",
              "      <td>0.10651</td>\n",
              "      <td>0.074024</td>\n",
              "      <td>0.037671</td>\n",
              "      <td>0.012951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.999500</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.254256</td>\n",
              "      <td>0.584127</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.521517</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>1.733092e-05</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.002001</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.199666</td>\n",
              "      <td>0.083025</td>\n",
              "      <td>0.002260</td>\n",
              "      <td>0.556050</td>\n",
              "      <td>0.423416</td>\n",
              "      <td>0.768509</td>\n",
              "      <td>0.281619</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.522557</td>\n",
              "      <td>0.540696</td>\n",
              "      <td>2.612877e-05</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.151372</td>\n",
              "      <td>0.639791</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.115297</td>\n",
              "      <td>0.625850</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.362497</td>\n",
              "      <td>0.786755</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.107714</td>\n",
              "      <td>0.656519</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.082125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.406011</td>\n",
              "      <td>0.719721</td>\n",
              "      <td>0.676839</td>\n",
              "      <td>0.658282</td>\n",
              "      <td>0.570998</td>\n",
              "      <td>0.504494</td>\n",
              "      <td>0.469920</td>\n",
              "      <td>0.722186</td>\n",
              "      <td>0.718519</td>\n",
              "      <td>0.661594</td>\n",
              "      <td>0.765062</td>\n",
              "      <td>0.724534</td>\n",
              "      <td>0.743533</td>\n",
              "      <td>0.521495</td>\n",
              "      <td>0.796331</td>\n",
              "      <td>0.830962</td>\n",
              "      <td>0.226037</td>\n",
              "      <td>0.500712</td>\n",
              "      <td>0.999925</td>\n",
              "      <td>0.494814</td>\n",
              "      <td>0.557812</td>\n",
              "      <td>0.438766</td>\n",
              "      <td>0.532593</td>\n",
              "      <td>0.351027</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073192</td>\n",
              "      <td>0.855168</td>\n",
              "      <td>0.782946</td>\n",
              "      <td>0.322375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.689239</td>\n",
              "      <td>0.144160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.999549</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.254136</td>\n",
              "      <td>0.584092</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.478925</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>9.995044e-07</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.244112</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.400148</td>\n",
              "      <td>0.423389</td>\n",
              "      <td>0.778023</td>\n",
              "      <td>0.280424</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.522652</td>\n",
              "      <td>0.541244</td>\n",
              "      <td>5.856015e-07</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.041995</td>\n",
              "      <td>0.507850</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.091036</td>\n",
              "      <td>0.415336</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.052037</td>\n",
              "      <td>0.550535</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.037165</td>\n",
              "      <td>0.518947</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.130274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.403543</td>\n",
              "      <td>0.749732</td>\n",
              "      <td>0.538677</td>\n",
              "      <td>0.517561</td>\n",
              "      <td>0.550497</td>\n",
              "      <td>0.404066</td>\n",
              "      <td>0.455401</td>\n",
              "      <td>0.745436</td>\n",
              "      <td>0.696357</td>\n",
              "      <td>0.495412</td>\n",
              "      <td>0.721633</td>\n",
              "      <td>0.618480</td>\n",
              "      <td>0.402165</td>\n",
              "      <td>0.322985</td>\n",
              "      <td>0.299568</td>\n",
              "      <td>0.534706</td>\n",
              "      <td>0.221800</td>\n",
              "      <td>0.477307</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.462956</td>\n",
              "      <td>0.553703</td>\n",
              "      <td>0.438418</td>\n",
              "      <td>0.469489</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.175719</td>\n",
              "      <td>0.902397</td>\n",
              "      <td>0.772610</td>\n",
              "      <td>0.289545</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.508576</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.999544</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.254202</td>\n",
              "      <td>0.584096</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.496427</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>1.006876e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.242935</td>\n",
              "      <td>0.165714</td>\n",
              "      <td>0.011115</td>\n",
              "      <td>0.370652</td>\n",
              "      <td>0.423439</td>\n",
              "      <td>0.710404</td>\n",
              "      <td>0.280857</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.522517</td>\n",
              "      <td>0.540718</td>\n",
              "      <td>6.123004e-05</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.087986</td>\n",
              "      <td>0.605132</td>\n",
              "      <td>0.000872</td>\n",
              "      <td>0.001245</td>\n",
              "      <td>0.214793</td>\n",
              "      <td>0.698926</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.122998</td>\n",
              "      <td>0.662052</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.113478</td>\n",
              "      <td>0.651057</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.110393</td>\n",
              "      <td>...</td>\n",
              "      <td>0.319761</td>\n",
              "      <td>0.405917</td>\n",
              "      <td>0.350358</td>\n",
              "      <td>0.360242</td>\n",
              "      <td>0.414859</td>\n",
              "      <td>0.252983</td>\n",
              "      <td>0.359081</td>\n",
              "      <td>0.470678</td>\n",
              "      <td>0.337911</td>\n",
              "      <td>0.379686</td>\n",
              "      <td>0.309799</td>\n",
              "      <td>0.456261</td>\n",
              "      <td>0.298250</td>\n",
              "      <td>0.246869</td>\n",
              "      <td>0.262793</td>\n",
              "      <td>0.332396</td>\n",
              "      <td>0.240564</td>\n",
              "      <td>0.490861</td>\n",
              "      <td>0.999923</td>\n",
              "      <td>0.465660</td>\n",
              "      <td>0.551967</td>\n",
              "      <td>0.433714</td>\n",
              "      <td>0.937287</td>\n",
              "      <td>0.596220</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.072902</td>\n",
              "      <td>0.854978</td>\n",
              "      <td>0.772610</td>\n",
              "      <td>0.471515</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.794392</td>\n",
              "      <td>0.001901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 368 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   flux_min  flux_max  flux_mean  ...   tflux_i   tflux_z   tflux_y\n",
              "0  0.999515  0.000342   0.254149  ...  0.000000  0.000000  0.000000\n",
              "1  0.994485  0.000334   0.253550  ...  0.074024  0.037671  0.012951\n",
              "2  0.999500  0.000466   0.254256  ...  0.000000  0.000000  0.000000\n",
              "3  0.999549  0.000319   0.254136  ...  0.000000  0.000000  0.000000\n",
              "4  0.999544  0.000379   0.254202  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[5 rows x 368 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8RiVsAV5ib5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)   #33-67\n",
        "clfs = []\n",
        "importances = pd.DataFrame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RVNRsdW5m6n",
        "colab_type": "code",
        "outputId": "0db83ed5-a223-4bed-f636-fd1aae3b7894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "classes = np.unique(y_train)\n",
        "print(classes.shape)\n",
        "classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cktxr6565p6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim_learning_rate = Real(low=1e-10, high=1e-1, prior='log-uniform',name='learning_rate')\n",
        "dim_estimators = Integer(low=50, high=3000,name='n_estimators')\n",
        "dim_max_depth = Integer(low=1, high=5,name='max_depth')\n",
        "\n",
        "dimensions = [dim_learning_rate,\n",
        "              dim_estimators,\n",
        "              dim_max_depth]\n",
        "\n",
        "default_parameters = [0.003,1000,2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvFMLqKN529s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def createModel(learning_rate,n_estimators,max_depth):       \n",
        "\n",
        "    oof_preds = np.zeros((len(x_train), len(classes)))\n",
        "    for fold_, (trn_, val_) in enumerate(folds.split(y_train, y_train)):\n",
        "        trn_x, trn_y = x_train.iloc[trn_], y_train.iloc[trn_]\n",
        "        val_x, val_y = x_train.iloc[val_], y_train.iloc[val_]\n",
        "\n",
        "        clf = lgb.LGBMClassifier(**lgb_params,learning_rate=learning_rate,\n",
        "                                n_estimators=n_estimators,max_depth=max_depth)\n",
        "        \n",
        "        \n",
        "        clf.fit(\n",
        "            trn_x, trn_y,\n",
        "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
        "            eval_metric=lgb_multi_weighted_logloss,\n",
        "            verbose=False,\n",
        "            early_stopping_rounds=50\n",
        "        )\n",
        "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
        "        print('fold',fold_+1,multi_weighted_logloss(val_y, clf.predict_proba(val_x, num_iteration=clf.best_iteration_)))\n",
        "\n",
        "        clfs.append(clf)\n",
        "        \n",
        "    \n",
        "    \n",
        "    loss = multi_weighted_logloss(y_true=y_train, y_preds=oof_preds)\n",
        "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % loss)\n",
        "    \n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlrxS4rM56UB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@use_named_args(dimensions=dimensions)\n",
        "def fitness(learning_rate,n_estimators,max_depth):\n",
        "    \"\"\"\n",
        "    Hyper-parameters:\n",
        "    learning_rate:     Learning-rate for the optimizer.\n",
        "    n_estimators:      Number of estimators.\n",
        "    max_depth:         Maximum Depth of tree.\n",
        "    \"\"\"\n",
        "\n",
        "    # Print the hyper-parameters.\n",
        "    print('learning rate: {0:.2e}'.format(learning_rate))\n",
        "    print('estimators:', n_estimators)\n",
        "    print('max depth:', max_depth)\n",
        "    \n",
        "    lv= createModel(learning_rate=learning_rate,\n",
        "                    n_estimators=n_estimators,\n",
        "                    max_depth = max_depth)\n",
        "    return lv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmxJXGwh59TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgb_params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 14,\n",
        "    'metric': 'multi_logloss',\n",
        "    'subsample': .9,\n",
        "    'colsample_bytree': .7,\n",
        "    'reg_alpha': .01,\n",
        "    'reg_lambda': .01,\n",
        "    'min_split_gain': 0.01,\n",
        "    'min_child_weight': 10,\n",
        "    'silent':True,\n",
        "    'verbosity':-1,\n",
        "    'nthread':-1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrPHn6gt6BPf",
        "colab_type": "code",
        "outputId": "d761929c-8b90-426f-a608-571bd0669d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "error = fitness(default_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate: 3.00e-03\n",
            "estimators: 1000\n",
            "max depth: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 4.346528828067377\n",
            "fold 2 4.335120682484117\n",
            "fold 3 4.354652776352549\n",
            "fold 4 4.350541849146568\n",
            "fold 5 4.3280583928953344\n",
            "MULTI WEIGHTED LOG LOSS : 4.34298 \n",
            "CPU times: user 15min 45s, sys: 3.93 s, total: 15min 49s\n",
            "Wall time: 8min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8KWhvlo6EjF",
        "colab_type": "code",
        "outputId": "9d71f5ad-698a-4614-a601-6e2fc6795c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "# use only if you haven't found out the optimal parameters for xgb. else comment this block.\n",
        "search_result = gp_minimize(func=fitness,\n",
        "                            dimensions=dimensions,\n",
        "                            acq_func='EI', # Expected Improvement.\n",
        "                            n_calls=11,\n",
        "                           x0=default_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate: 3.00e-03\n",
            "estimators: 1000\n",
            "max depth: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 4.346528828067377\n",
            "fold 2 4.335120682484117\n",
            "fold 3 4.354652776352549\n",
            "fold 4 4.350541849146568\n",
            "fold 5 4.3280583928953344\n",
            "MULTI WEIGHTED LOG LOSS : 4.34298 \n",
            "learning rate: 4.62e-09\n",
            "estimators: 1839\n",
            "max depth: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.2338022740976946\n",
            "fold 2 3.232929953438124\n",
            "fold 3 3.2323816201602087\n",
            "fold 4 3.2284339749626523\n",
            "fold 5 3.2282286461343985\n",
            "MULTI WEIGHTED LOG LOSS : 3.23116 \n",
            "learning rate: 3.46e-07\n",
            "estimators: 2747\n",
            "max depth: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.2338364564970146\n",
            "fold 2 3.2329712749837403\n",
            "fold 3 3.2324120200299697\n",
            "fold 4 3.2284718047045695\n",
            "fold 5 3.228251938508767\n",
            "MULTI WEIGHTED LOG LOSS : 3.23120 \n",
            "learning rate: 6.30e-05\n",
            "estimators: 2351\n",
            "max depth: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.2477444915341525\n",
            "fold 2 3.2489404364028807\n",
            "fold 3 3.247824757129476\n",
            "fold 4 3.2446813204978713\n",
            "fold 5 3.2423532767130854\n",
            "MULTI WEIGHTED LOG LOSS : 3.24632 \n",
            "learning rate: 3.63e-05\n",
            "estimators: 1647\n",
            "max depth: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.2379081114062047\n",
            "fold 2 3.2374170190608718\n",
            "fold 3 3.2365190049724726\n",
            "fold 4 3.2330391749241\n",
            "fold 5 3.2318135614954535\n",
            "MULTI WEIGHTED LOG LOSS : 3.23535 \n",
            "learning rate: 2.52e-02\n",
            "estimators: 2939\n",
            "max depth: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 6.629091902378908\n",
            "fold 2 6.587043344271814\n",
            "fold 3 6.642121950035489\n",
            "fold 4 6.62066379698397\n",
            "fold 5 6.397836776353318\n",
            "MULTI WEIGHTED LOG LOSS : 6.57550 \n",
            "learning rate: 2.27e-06\n",
            "estimators: 281\n",
            "max depth: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.2338252089431174\n",
            "fold 2 3.2329577446843927\n",
            "fold 3 3.2324023273452944\n",
            "fold 4 3.22845933769281\n",
            "fold 5 3.2282448456990367\n",
            "MULTI WEIGHTED LOG LOSS : 3.23119 \n",
            "learning rate: 1.28e-05\n",
            "estimators: 1161\n",
            "max depth: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.2344051640454383\n",
            "fold 2 3.2336162281777057\n",
            "fold 3 3.232903503789838\n",
            "fold 4 3.229150722826456\n",
            "fold 5 3.228642645660314\n",
            "MULTI WEIGHTED LOG LOSS : 3.23175 \n",
            "learning rate: 7.48e-02\n",
            "estimators: 2664\n",
            "max depth: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 6.581715561571828\n",
            "fold 2 6.650117052680621\n",
            "fold 3 6.680443920160496\n",
            "fold 4 6.691675784533611\n",
            "fold 5 6.53361758640572\n",
            "MULTI WEIGHTED LOG LOSS : 6.62751 \n",
            "learning rate: 1.89e-04\n",
            "estimators: 327\n",
            "max depth: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.2372317611093324\n",
            "fold 2 3.237330906724452\n",
            "fold 3 3.2363332762519397\n",
            "fold 4 3.232838070410781\n",
            "fold 5 3.23162096803928\n",
            "MULTI WEIGHTED LOG LOSS : 3.23508 \n",
            "learning rate: 1.36e-04\n",
            "estimators: 1491\n",
            "max depth: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.263297125126227\n",
            "fold 2 3.263794021758346\n",
            "fold 3 3.262136199282202\n",
            "fold 4 3.258947560559827\n",
            "fold 5 3.256125854038667\n",
            "MULTI WEIGHTED LOG LOSS : 3.26087 \n",
            "CPU times: user 4h 36min 47s, sys: 1min 11s, total: 4h 37min 59s\n",
            "Wall time: 2h 20min 15s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiRL1TFTBMkI",
        "colab_type": "code",
        "outputId": "77f530a0-c971-48fb-96d0-12fd4a64ca65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        " plot_convergence(search_result)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEYCAYAAABGJWFlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXpWqH81txK2",
        "colab_type": "code",
        "outputId": "a1fdcce9-900b-4d1f-b188-596e73d5a2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# optimal parameters found using scikit optimize. use these parameter to initialize the 2nd level model.\n",
        "print(search_result.x)\n",
        "learning_rate = search_result.x[0]\n",
        "n_estimators = search_result.x[1]\n",
        "max_depth = search_result.x[2]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.618820172959631e-09, 1839, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK2JQ1uDt2wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "op_h_p = [1e-08,50,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWfPjpdquEjX",
        "colab_type": "code",
        "outputId": "6215570e-3bfb-4366-c9ba-e008ede075a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "joblib.dump(fitness(op_h_p),'model3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate: 1.00e-08\n",
            "estimators: 50\n",
            "max depth: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 3.233802198532993\n",
            "fold 2 3.2329297943089426\n",
            "fold 3 3.232381535598546\n",
            "fold 4 3.2284338693273344\n",
            "fold 5 3.228228656466425\n",
            "MULTI WEIGHTED LOG LOSS : 3.23116 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6nRoqOAuOLa",
        "colab_type": "code",
        "outputId": "7dccd7d5-2e98-41b5-d644-838e94aa5f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "oof_preds = np.zeros((len(x_train), len(classes)))\n",
        "for fold_, (trn_, val_) in enumerate(folds.split(y_train, y_train)):\n",
        "    trn_x, trn_y = x_train.iloc[trn_], y_train.iloc[trn_]\n",
        "    val_x, val_y = x_train.iloc[val_], y_train.iloc[val_]\n",
        "    \n",
        "    clf = lgb.LGBMClassifier(**lgb_params,learning_rate=learning_rate,\n",
        "                                n_estimators=n_estimators,max_depth=max_depth)\n",
        "    clf.fit(\n",
        "        trn_x, trn_y,\n",
        "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
        "        eval_metric=lgb_multi_weighted_logloss,\n",
        "        verbose=100,\n",
        "        early_stopping_rounds=50\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
        "    print(multi_weighted_logloss(val_y, clf.predict_proba(val_x, num_iteration=clf.best_iteration_)))\n",
        "\n",
        "    clfs.append(clf)\n",
        "\n",
        "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y_train, y_preds=oof_preds))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.2338\tvalid_1's multi_logloss: 2.18257\tvalid_1's wloss: 3.2338\n",
            "[200]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.2338\tvalid_1's multi_logloss: 2.18257\tvalid_1's wloss: 3.2338\n",
            "[300]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.2338\tvalid_1's multi_logloss: 2.18257\tvalid_1's wloss: 3.2338\n",
            "[400]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.2338\tvalid_1's multi_logloss: 2.18257\tvalid_1's wloss: 3.2338\n",
            "[500]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.2338\tvalid_1's multi_logloss: 2.18257\tvalid_1's wloss: 3.2338\n",
            "[600]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18257\tvalid_1's wloss: 3.2338\n",
            "[700]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[800]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[900]\ttraining's multi_logloss: 2.1755\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[1000]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[1100]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[1200]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[1300]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[1400]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23379\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23379\n",
            "[1500]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23378\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23378\n",
            "[1600]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23378\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23378\n",
            "[1700]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23378\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23378\n",
            "[1800]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23378\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23378\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1839]\ttraining's multi_logloss: 2.17549\ttraining's wloss: 3.23378\tvalid_1's multi_logloss: 2.18256\tvalid_1's wloss: 3.23378\n",
            "3.2338022740976946\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 2.17623\ttraining's wloss: 3.23293\tvalid_1's multi_logloss: 2.17968\tvalid_1's wloss: 3.23293\n",
            "[200]\ttraining's multi_logloss: 2.17623\ttraining's wloss: 3.23293\tvalid_1's multi_logloss: 2.17968\tvalid_1's wloss: 3.23293\n",
            "[300]\ttraining's multi_logloss: 2.17623\ttraining's wloss: 3.23293\tvalid_1's multi_logloss: 2.17968\tvalid_1's wloss: 3.23293\n",
            "[400]\ttraining's multi_logloss: 2.17623\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17968\tvalid_1's wloss: 3.23293\n",
            "[500]\ttraining's multi_logloss: 2.17623\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[600]\ttraining's multi_logloss: 2.17623\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[700]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[800]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[900]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[1000]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[1100]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[1200]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23292\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23292\n",
            "[1300]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23291\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23291\n",
            "[1400]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23291\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23291\n",
            "[1500]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23291\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23291\n",
            "[1600]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23291\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23291\n",
            "[1700]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23291\tvalid_1's multi_logloss: 2.17967\tvalid_1's wloss: 3.23291\n",
            "[1800]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23291\tvalid_1's multi_logloss: 2.17966\tvalid_1's wloss: 3.23291\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1839]\ttraining's multi_logloss: 2.17622\ttraining's wloss: 3.23291\tvalid_1's multi_logloss: 2.17966\tvalid_1's wloss: 3.23291\n",
            "3.232929953438124\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23238\tvalid_1's multi_logloss: 2.17664\tvalid_1's wloss: 3.23238\n",
            "[200]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23238\tvalid_1's multi_logloss: 2.17664\tvalid_1's wloss: 3.23238\n",
            "[300]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23238\tvalid_1's multi_logloss: 2.17664\tvalid_1's wloss: 3.23238\n",
            "[400]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23238\tvalid_1's multi_logloss: 2.17664\tvalid_1's wloss: 3.23238\n",
            "[500]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23238\tvalid_1's multi_logloss: 2.17664\tvalid_1's wloss: 3.23238\n",
            "[600]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17664\tvalid_1's wloss: 3.23237\n",
            "[700]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17664\tvalid_1's wloss: 3.23237\n",
            "[800]\ttraining's multi_logloss: 2.17699\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23237\n",
            "[900]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23237\n",
            "[1000]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23237\n",
            "[1100]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23237\n",
            "[1200]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23237\n",
            "[1300]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23237\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23237\n",
            "[1400]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23236\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23237\n",
            "[1500]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23236\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23236\n",
            "[1600]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23236\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23236\n",
            "[1700]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23236\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23236\n",
            "[1800]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23236\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23236\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1839]\ttraining's multi_logloss: 2.17698\ttraining's wloss: 3.23236\tvalid_1's multi_logloss: 2.17663\tvalid_1's wloss: 3.23236\n",
            "3.2323816201602087\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 2.17787\ttraining's wloss: 3.22843\tvalid_1's multi_logloss: 2.17313\tvalid_1's wloss: 3.22843\n",
            "[200]\ttraining's multi_logloss: 2.17787\ttraining's wloss: 3.22843\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22843\n",
            "[300]\ttraining's multi_logloss: 2.17787\ttraining's wloss: 3.22843\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22843\n",
            "[400]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22843\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22843\n",
            "[500]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22843\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22843\n",
            "[600]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22843\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22843\n",
            "[700]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22843\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22843\n",
            "[800]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22842\n",
            "[900]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22842\n",
            "[1000]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22842\n",
            "[1100]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22842\n",
            "[1200]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22842\n",
            "[1300]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22842\n",
            "[1400]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17312\tvalid_1's wloss: 3.22842\n",
            "[1500]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22842\tvalid_1's multi_logloss: 2.17311\tvalid_1's wloss: 3.22842\n",
            "[1600]\ttraining's multi_logloss: 2.17786\ttraining's wloss: 3.22841\tvalid_1's multi_logloss: 2.17311\tvalid_1's wloss: 3.22841\n",
            "[1700]\ttraining's multi_logloss: 2.17785\ttraining's wloss: 3.22841\tvalid_1's multi_logloss: 2.17311\tvalid_1's wloss: 3.22841\n",
            "[1800]\ttraining's multi_logloss: 2.17785\ttraining's wloss: 3.22841\tvalid_1's multi_logloss: 2.17311\tvalid_1's wloss: 3.22841\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1839]\ttraining's multi_logloss: 2.17785\ttraining's wloss: 3.22841\tvalid_1's multi_logloss: 2.17311\tvalid_1's wloss: 3.22841\n",
            "3.2284339749626523\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 2.17799\ttraining's wloss: 3.22823\tvalid_1's multi_logloss: 2.17263\tvalid_1's wloss: 3.22823\n",
            "[200]\ttraining's multi_logloss: 2.17799\ttraining's wloss: 3.22823\tvalid_1's multi_logloss: 2.17263\tvalid_1's wloss: 3.22823\n",
            "[300]\ttraining's multi_logloss: 2.17799\ttraining's wloss: 3.22823\tvalid_1's multi_logloss: 2.17263\tvalid_1's wloss: 3.22823\n",
            "[400]\ttraining's multi_logloss: 2.17799\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17263\tvalid_1's wloss: 3.22822\n",
            "[500]\ttraining's multi_logloss: 2.17799\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22822\n",
            "[600]\ttraining's multi_logloss: 2.17799\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22822\n",
            "[700]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22822\n",
            "[800]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22822\n",
            "[900]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22822\n",
            "[1000]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22822\n",
            "[1100]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22822\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22822\n",
            "[1200]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22821\n",
            "[1300]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22821\n",
            "[1400]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22821\n",
            "[1500]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22821\n",
            "[1600]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22821\n",
            "[1700]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17262\tvalid_1's wloss: 3.22821\n",
            "[1800]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17261\tvalid_1's wloss: 3.22821\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1839]\ttraining's multi_logloss: 2.17798\ttraining's wloss: 3.22821\tvalid_1's multi_logloss: 2.17261\tvalid_1's wloss: 3.22821\n",
            "3.2282286461343985\n",
            "MULTI WEIGHTED LOG LOSS : 3.23116 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuARuuVluUfw",
        "colab_type": "code",
        "outputId": "e94045c5-05f5-49fd-a47c-4326a0c65c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "print(oof_preds.shape)\n",
        "oof_preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5886, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01972832, 0.06364146, 0.11752143, ..., 0.29359385, 0.03139561,\n",
              "        0.02142669],\n",
              "       [0.01974932, 0.06349678, 0.11764639, ..., 0.29390619, 0.03142899,\n",
              "        0.02144818],\n",
              "       [0.01973261, 0.06344129, 0.11754683, ..., 0.29365732, 0.03140239,\n",
              "        0.02143002],\n",
              "       ...,\n",
              "       [0.01974942, 0.06349533, 0.11764694, ..., 0.29390484, 0.03142914,\n",
              "        0.02144828],\n",
              "       [0.01974938, 0.063497  , 0.11764671, ..., 0.29390435, 0.03142908,\n",
              "        0.02144824],\n",
              "       [0.01976615, 0.06354921, 0.11753422, ..., 0.29415669, 0.0314558 ,\n",
              "        0.02146647]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Tte7L2wdT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = None\n",
        "for clf in clfs:\n",
        "    if preds is None:\n",
        "        preds = clf.predict_proba(x_test[x_train.columns]) / folds.n_splits\n",
        "    else:\n",
        "        preds += clf.predict_proba(x_test[x_train.columns]) / folds.n_splits\n",
        "    \n",
        "\n",
        "    \n",
        "# Store predictions\n",
        "preds_df = pd.DataFrame(preds, columns=['class_' + str(s) for s in clfs[0].classes_])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9S85GU1wgs1",
        "colab_type": "code",
        "outputId": "82914124-01c8-4992-c989-706c7dc79608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "print(preds_df.shape)\n",
        "preds_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1962, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_6</th>\n",
              "      <th>class_15</th>\n",
              "      <th>class_16</th>\n",
              "      <th>class_42</th>\n",
              "      <th>class_52</th>\n",
              "      <th>class_53</th>\n",
              "      <th>class_62</th>\n",
              "      <th>class_64</th>\n",
              "      <th>class_65</th>\n",
              "      <th>class_67</th>\n",
              "      <th>class_88</th>\n",
              "      <th>class_90</th>\n",
              "      <th>class_92</th>\n",
              "      <th>class_95</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.201888</td>\n",
              "      <td>0.644103</td>\n",
              "      <td>1.191709</td>\n",
              "      <td>2.003967</td>\n",
              "      <td>0.312917</td>\n",
              "      <td>0.031867</td>\n",
              "      <td>0.686778</td>\n",
              "      <td>0.138443</td>\n",
              "      <td>1.276563</td>\n",
              "      <td>0.318006</td>\n",
              "      <td>0.464247</td>\n",
              "      <td>6.193396</td>\n",
              "      <td>0.318643</td>\n",
              "      <td>0.217474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.217493</td>\n",
              "      <td>0.630749</td>\n",
              "      <td>5.187631</td>\n",
              "      <td>1.508563</td>\n",
              "      <td>0.225435</td>\n",
              "      <td>0.035766</td>\n",
              "      <td>0.635495</td>\n",
              "      <td>0.135556</td>\n",
              "      <td>1.269807</td>\n",
              "      <td>0.277201</td>\n",
              "      <td>0.455735</td>\n",
              "      <td>2.894137</td>\n",
              "      <td>0.313320</td>\n",
              "      <td>0.213112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.202045</td>\n",
              "      <td>0.657334</td>\n",
              "      <td>1.193092</td>\n",
              "      <td>1.984523</td>\n",
              "      <td>0.321404</td>\n",
              "      <td>0.034092</td>\n",
              "      <td>0.761823</td>\n",
              "      <td>0.139235</td>\n",
              "      <td>1.273110</td>\n",
              "      <td>0.350457</td>\n",
              "      <td>0.466788</td>\n",
              "      <td>6.076173</td>\n",
              "      <td>0.320800</td>\n",
              "      <td>0.219126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.200871</td>\n",
              "      <td>0.641028</td>\n",
              "      <td>1.179546</td>\n",
              "      <td>2.024472</td>\n",
              "      <td>0.296268</td>\n",
              "      <td>0.031570</td>\n",
              "      <td>0.702160</td>\n",
              "      <td>0.182754</td>\n",
              "      <td>1.258979</td>\n",
              "      <td>0.350065</td>\n",
              "      <td>0.477579</td>\n",
              "      <td>6.111820</td>\n",
              "      <td>0.316818</td>\n",
              "      <td>0.226072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.204065</td>\n",
              "      <td>0.655085</td>\n",
              "      <td>1.198180</td>\n",
              "      <td>2.128161</td>\n",
              "      <td>0.527963</td>\n",
              "      <td>0.034523</td>\n",
              "      <td>0.880167</td>\n",
              "      <td>0.140375</td>\n",
              "      <td>1.278959</td>\n",
              "      <td>0.349319</td>\n",
              "      <td>0.471825</td>\n",
              "      <td>5.586423</td>\n",
              "      <td>0.323155</td>\n",
              "      <td>0.221800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    class_6  class_15  class_16  ...  class_90  class_92  class_95\n",
              "0  0.201888  0.644103  1.191709  ...  6.193396  0.318643  0.217474\n",
              "1  0.217493  0.630749  5.187631  ...  2.894137  0.313320  0.213112\n",
              "2  0.202045  0.657334  1.193092  ...  6.076173  0.320800  0.219126\n",
              "3  0.200871  0.641028  1.179546  ...  6.111820  0.316818  0.226072\n",
              "4  0.204065  0.655085  1.198180  ...  5.586423  0.323155  0.221800\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R8Z2FNLJg1x",
        "colab_type": "code",
        "outputId": "ba598a7f-5230-494f-aa90-f717a65a415e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "y_test.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[90],\n",
              "       [16],\n",
              "       [90],\n",
              "       ...,\n",
              "       [88],\n",
              "       [42],\n",
              "       [88]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hh3zECAxSAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "    \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edI8osgZxWXf",
        "colab_type": "code",
        "outputId": "55761804-8952-43a1-cc36-b04d1593b6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "y_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target\n",
              "0      90\n",
              "1      16\n",
              "2      90\n",
              "3      90\n",
              "4      52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gy91HKY0Wt9",
        "colab_type": "code",
        "outputId": "d6abc929-3318-4342-bf0d-f819016988d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "y_test = y_test['target']\n",
        "y_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    90\n",
              "1    16\n",
              "2    90\n",
              "3    90\n",
              "4    52\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I50UyIcJxaJg",
        "colab_type": "code",
        "outputId": "88fba034-8bad-4d9a-f74b-5db4cc42d40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1962, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rASRv8mvJ1Zj",
        "colab_type": "code",
        "outputId": "e73246e0-53fc-4124-e26c-36f609a37232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.20188771, 0.64410277, 1.19170865, ..., 6.19339555, 0.31864268,\n",
              "        0.21747427],\n",
              "       [0.21749252, 0.63074896, 5.1876311 , ..., 2.89413731, 0.31332006,\n",
              "        0.21311247],\n",
              "       [0.20204464, 0.65733375, 1.19309206, ..., 6.07617251, 0.32079983,\n",
              "        0.21912621],\n",
              "       ...,\n",
              "       [0.19525271, 0.62549175, 1.16202204, ..., 3.00211964, 0.30972456,\n",
              "        0.22010762],\n",
              "       [0.19642636, 0.63981716, 1.16465754, ..., 3.63151344, 0.31260864,\n",
              "        0.21317771],\n",
              "       [0.24728645, 0.61483154, 1.25450376, ..., 2.95124886, 0.3324049 ,\n",
              "        0.3035069 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWdK3QewxdVO",
        "colab_type": "code",
        "outputId": "0719499a-4e42-4186-ddee-aaa71506d5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "y_d = pd.get_dummies(y_test)\n",
        "print(y_d.shape)\n",
        "y_d.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1962, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIPwYfDFxf5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(np.argmax(y_d.values,axis=-1), np.argmax(preds,axis=-1))\n",
        "np.set_printoptions(precision=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfH3CPtqxmki",
        "colab_type": "code",
        "outputId": "93903982-fa61-4e0e-c3f7-d18a5833b200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(12,12))\n",
        "foo = plot_confusion_matrix(cnf_matrix, classes,normalize=True,\n",
        "                      title='Confusion matrix')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized confusion matrix\n",
            "[[0.89 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.11 0.   0.  ]\n",
            " [0.   0.93 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.07 0.   0.  ]\n",
            " [0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.47 0.   0.   0.   0.   0.   0.   0.   0.53 0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            " [0.   0.   0.   0.01 0.   0.   0.62 0.   0.   0.   0.   0.38 0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.64 0.   0.   0.   0.36 0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.88 0.12 0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            " [0.   0.   0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.02 0.96 0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.18 0.   0.82]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAx8AAANYCAYAAACl+VBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwCsZMU5+4fLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gsuxDgKxvcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(confusion_matrix):\n",
        "    diagonal_sum = np.trace(cnf_matrix)\n",
        "    sum_of_all_elements = np.sum(cnf_matrix)\n",
        "    return diagonal_sum / sum_of_all_elements \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4cnK8ODKOB4",
        "colab_type": "code",
        "outputId": "4151d269-26d5-4e0d-f402-a1bc4ca74265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "accuracy(confusion_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8190621814475025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkZZF0IqKRY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}